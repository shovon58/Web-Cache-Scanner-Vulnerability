package pkg

import (
	"bytes"
	"fmt"
	"io/ioutil"
	"net/http"
	"net/url"
	"os"
	"strings"
)

func init() {

}

func CheckCache(client http.Client, config Config) Cache {
	/*
		Wie kann ich erkennen, dass ein Cache aktiv ist?
		- X-Cache : hit / miss
		- Cf-Cache-Status
		- Zeit messen
	*/
	/*
		Was kann ich als Cache Buster benutzen?
		- Query-Parameter (cachebuster)
		- Header (z.B. origin)
	*/
	var cache Cache

	for key, val := range config.Website.Headers {
		switch strings.ToLower(key) {
		case "cache-control", "pragma":
			for _, val2 := range val {
				switch strings.ToLower(val2) {
				case "no-cache":
					msg := fmt.Sprintf("%s is set to no-cache, so likely the site can't be tested for web cache poisoning. But some caches don't honor this header\n", key)
					PrintVerbose(msg, Yellow, 1)
				case "no-store":
					msg := fmt.Sprintf("%s is set to no-store, so likely the site can't be tested for web cache poisoning. But some caches don't honor this header\n", key)
					PrintVerbose(msg, Yellow, 1)
				case "only-if-cached":
					msg := fmt.Sprintf("%s is set to only-if-cached, so likely the site can't be tested for web cache poisoning. But some caches don't honor this header\n", key)
					PrintVerbose(msg, Yellow, 1)
				}
			}
		case "x-cache":
			cache.Indicator = key
			msg := fmt.Sprintf("%s header was found \n", key)
			PrintVerbose(msg, NoColor, 1)
		case "cf-cache-status":
			cache.Indicator = key
			msg := fmt.Sprintf("%s header was found \n", key)
			PrintVerbose(msg, NoColor, 1)
		}
	}

	if cache.Indicator == "" {
		msg := "The x-cache header wasn't found\n"
		Print(msg, Yellow)
		if !config.Force {
			os.Exit(1)
		}
	}

	urlCb, _ := addCacheBuster(config.Website.Url.String(), "", config.CacheBuster)

	var req *http.Request
	var err error
	if config.DoPost {
		req, err = http.NewRequest("POST", urlCb, bytes.NewBufferString(config.Body))
	} else {
		req, err = http.NewRequest("GET", urlCb, nil)
	}
	if err != nil {
		msg := err.Error() + "\n"
		PrintFatal(msg)
	}

	setRequest(req, config.DoPost, config)

	resp, err := client.Do(req)
	if err != nil {
		msg := err.Error() + "\n"
		PrintFatal(msg)
	}
	defer resp.Body.Close()

	if resp.StatusCode != config.Website.StatusCode {
		msg := fmt.Sprintf("Unexpected Status Code %d\n", resp.StatusCode)
		Print(msg, Yellow)
	}

	if strings.ToLower(resp.Header.Get(cache.Indicator)) == "hit" {
		cache.Parameter = false
		msg := fmt.Sprintf("Parameter %s as CacheBuster is not possible\n", config.CacheBuster)
		Print(msg, Yellow)
	}

	urlCb, _ = addCacheBuster(config.Website.Url.String(), "", config.CacheBuster)

	if config.DoPost {
		req, err = http.NewRequest("POST", urlCb, bytes.NewBufferString(config.Body))
	} else {
		req, err = http.NewRequest("GET", urlCb, nil)
	}
	if err != nil {
		msg := err.Error() + "\n"
		PrintFatal(msg)
	}

	setRequest(req, config.DoPost, config)
	resp, err = client.Do(req)
	if err != nil {
		msg := err.Error() + "\n"
		PrintFatal(msg)
	}
	defer resp.Body.Close()

	if resp.StatusCode != config.Website.StatusCode {
		msg := fmt.Sprintf("Unexpected Status Code %d\n", resp.StatusCode)
		Print(msg, Yellow)
	}

	if strings.ToLower(resp.Header.Get(cache.Indicator)) == "hit" {
		cache.Parameter = false
		msg := fmt.Sprintf("Parameter %s as CacheBuster is not possible\n", config.CacheBuster)
		Print(msg, Yellow)
	} else {
		cache.Parameter = true
		cache.ParameterName = config.CacheBuster
		msg := fmt.Sprintf("Parameter %s as CacheBuster was successful\n", config.CacheBuster)
		PrintVerbose(msg, NoColor, 1)
	}

	if !cache.Parameter && !config.Force {
		msg := "Please specify a successful cachebuster parameter or header\n"
		PrintFatal(msg)
	}

	return cache
}

/* Simple get request to get the body of a normal response and the cookies */
func GetWebsite(requrl string, client http.Client, retrieveCookies bool, firstRequest bool, config Config) Website {

	var cache Cache

	queryParameterMap := make(map[string]string)

	// get domain
	domainParts := strings.SplitN(requrl, "/", 4)
	domain := domainParts[0] + "//" + domainParts[2]

	// splitting url like {https://www.m10x.de/}?{name=max&role=admin}
	urlSlice := strings.SplitN(requrl, "?", 2)

	// splitting queries like {name=max}&{role=admin}
	var parameterSlice []string
	if strings.Contains(requrl, "?") {
		parameterSlice = strings.Split(urlSlice[1], config.QuerySeperator)
	}

	if len(parameterSlice) > 0 {
		queryParameterMap = setQueryParameterMap(queryParameterMap, parameterSlice)
	}

	if len(config.Parameters) > 0 {
		queryParameterMap = setQueryParameterMap(queryParameterMap, config.Parameters)
	}

	requrl = urlSlice[0] + "?"
	urlNoQueries := urlSlice[0]

	// adding query parameter
	for key, val := range queryParameterMap {
		if !strings.HasSuffix(requrl, "?") {
			requrl += "&"
		}
		requrl += key + "=" + val
	}

	if len(queryParameterMap) > 0 {
		requrl += config.QuerySeperator
	}

	// adding cachebuster
	//requrlcb, _ := addCacheBuster(requrl, "", config.CacheBuster)

	var req *http.Request
	var err error
	if config.DoPost {
		req, err = http.NewRequest("POST", requrl[:(len(requrl)-1)], bytes.NewBufferString(config.Body))
	} else {
		req, err = http.NewRequest("GET", requrl[:(len(requrl)-1)], nil)
	}
	if err != nil {
		msg := err.Error() + "\n"
		PrintFatal(msg)
	}

	setRequest(req, config.DoPost, config)
	resp, err := client.Do(req)
	if err != nil {
		msg := err.Error() + "\n"
		PrintFatal(msg)
	}

	defer resp.Body.Close()

	body, err := ioutil.ReadAll(resp.Body)
	if err != nil {
		msg := err.Error() + "\n"
		PrintFatal(msg)
	}

	weburl, err := url.Parse(requrl)
	if err != nil {
		msg := err.Error() + "\n"
		PrintFatal(msg)
	}

	tempStatusCode := config.StatusCode
	if tempStatusCode == 0 {
		tempStatusCode = resp.StatusCode

		msg := fmt.Sprintf("The default Status Code was set to %d\n", tempStatusCode)
		Print(msg, NoColor)
	}

	// if retrieveCookies is false, only the specified cookies will be used
	// otherwise the by the server given cookies AND the specified cookies will be used
	cookiesWebsite := config.Website.Cookies
	if retrieveCookies {
		cookiesWebsite = append(cookiesWebsite, resp.Cookies()...)
	}

	c := Website{
		Headers:    resp.Header,
		Body:       string(body),
		Cookies:    cookiesWebsite,
		StatusCode: tempStatusCode,
		Url:        weburl,
		BaseUrlStr: urlNoQueries,
		Queries:    queryParameterMap,
		Cache:      cache,
		Domain:     domain,
		//make map doesnt work here. is now in main method
		//Added:      make(map[string]bool),
	}

	return c
}

func setQueryParameterMap(queryParameterMap map[string]string, querySlice []string) map[string]string {
	for _, q := range querySlice {
		q = strings.TrimSuffix(q, "\r")
		q = strings.TrimSpace(q)
		if q == "" {
			continue
		} else if !strings.Contains(q, "=") {
			msg := fmt.Sprintf("Specified parameter %s doesn't contain a = and will be skipped\n", q)
			Print(msg, Yellow)
			continue
		} else {
			query := strings.SplitN(q, "=", 2)
			// ok is true, if a query already is set
			val, ok := queryParameterMap[query[0]]
			if ok {
				msg := fmt.Sprintf("Overwriting %s=%s with %s=%s\n", query[0], val, query[0], query[1])
				Print(msg, NoColor)
			}
			queryParameterMap[query[0]] = query[1]
		}
	}

	return queryParameterMap
}

func getSliceBetweenStrings(start string, start2 string, end string, all string) []string {
	startSlice := strings.Split(all, start)
	var betweenSlice []string

	for _, s := range startSlice {
		if strings.Contains(s, end) {
			tempSlice := strings.SplitN(s, end, 2)
			tempString := tempSlice[0]

			if start2 != "" {
				tempSlice = strings.SplitN(tempString, start2, 2)
				tempString = tempSlice[1]
			}

			betweenSlice = append(betweenSlice, tempString)
		}
	}

	return betweenSlice
}

func trimString(x string) string {
	x = strings.TrimSpace(x)
	if strings.HasPrefix(x, "'") || strings.HasSuffix(x, "'") {
		x = strings.Trim(x, "'")
	}
	if strings.HasPrefix(x, "\"") || strings.HasSuffix(x, "\"") {
		x = strings.Trim(x, "\"")
	}

	return x
}

func addDomain(x string, domain string) string {
	if strings.HasPrefix(x, domain) {
		return x
	} else if strings.HasPrefix(x, "/") {
		return domain + x
	} else {
		msg := fmt.Sprintf("%s is not a valid url or doesn't have the same domain %s\n", x, domain)
		PrintVerbose(msg, Yellow, 1)

		return ""
	}
}

func checkRecInclude(x string, recInclude string) bool {
	for _, inc := range strings.Split(recInclude, " ") {
		// remove spaces and skip if someone used multiple spaces instead of one
		// TODO: is it necessary to trim spaces here after splitting for spaces?
		inc = strings.TrimSpace(inc)
		if inc == "" {
			continue
		}
		if strings.Contains(x, inc) {
			return true
		}
	}
	return false
}

func CrawlUrls(config Config, added map[string]bool) []string {
	// search for a tags. Tags can have spaces at the end, so </a > would be possible but not < /a>
	aSlice := getSliceBetweenStrings("<a ", "", "</a", config.Website.Body)
	var hrefSlice []string
	for _, a := range aSlice {
		tempSlice := getSliceBetweenStrings("href", "=", ">", a)
		hrefSlice = append(hrefSlice, tempSlice...)
	}

	scriptSlice := getSliceBetweenStrings("<script ", "", "</script", config.Website.Body)
	var srcSlice []string
	for _, script := range scriptSlice {
		tempSlice := getSliceBetweenStrings("src", "=", ">", script)
		srcSlice = append(srcSlice, tempSlice...)
	}

	var urls []string

	for _, x := range append(hrefSlice, srcSlice...) {
		x = trimString(x)
		x = addDomain(x, config.Website.Domain)
		if x == "" {
			continue
		}
		// Check if url isnt added yet and if it satisfies RecInclude (=contains it)
		if added[x] {
			msg := fmt.Sprintf("Skipped to add %s to the test queue, because it was already added\n", x)
			PrintVerbose(msg, NoColor, 2)
			continue
		} else if config.RecInclude == "" || checkRecInclude(x, config.RecInclude) {
			urls = append(urls, x)
			added[x] = true
		} else {
			msg := fmt.Sprintf("Skipped to add %s to the test queue, because it doesn't satisfy RecInclude\n", x)
			PrintVerbose(msg, NoColor, 1)
		}
	}

	return urls
}
