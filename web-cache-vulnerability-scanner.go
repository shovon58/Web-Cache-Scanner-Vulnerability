package main

import (
	"bytes"
	"crypto/tls"
	"crypto/x509"
	"flag"
	"io"
	"io/ioutil"
	"log"
	"math/rand"
	"net/http"
	"net/http/cookiejar"
	"net/url"
	"os"
	"strconv"
	"strings"
	"sync"
	"time"
)

/*
Build module
env GOOS=windows GOARCH=amd64 go build
set GOOS=
set GOARCH=
go build

TODO: Specify cookies, query parameters, headers, body, support POST
*/
var threads int
var verbose bool
var quiet bool
var doPost bool
var contentType string
var querySeperator string
var cacheBuster string

var website Website
var givenBody string
var givenCookies []string
var givenHeaders []string
var givenParameters []string

var headersURL string
var parametersURL string
var topHeadersURL string
var topParametersURL string

type Website struct {
	Body       string
	Cookies    []*http.Cookie
	Status     string
	Url        *url.URL
	BaseUrlStr string
	Queries    map[string]string
}

func main() {
	/* Setting hardcoded global vars */
	headersURL = "https://raw.githubusercontent.com/m10x/wordlists/master/headers"
	parametersURL = "https://raw.githubusercontent.com/m10x/wordlists/master/parameters"
	topHeadersURL = "https://raw.githubusercontent.com/m10x/wordlists/master/top-headers"
	topParametersURL = "https://raw.githubusercontent.com/m10x/wordlists/master/top-parameters"
	/*********************************/

	/* Getting Command-line flags */
	urlPtr := flag.String("url", "", "Url to scan. Use urllist if you want to scan multiple urls")
	urlListPtr := flag.String("urllist", "", "List with urls to scan. One url per line")

	proxyCertPathPtr := flag.String("proxycertpath", "", "Path to the cert of the proxy you want to use. The cert has to have the PEM Format. Burp e.g. is in the DER Format. Use the following command to convert it: openssl x509 -inform DER -outform PEM -text -in cacert.der -out certificate.pem")
	proxyURLPtr := flag.String("proxyurl", "http://127.0.0.1:8080", "Url for the proxy. Default value is http://127.0.0.1:8080")

	threadsPtr := flag.Int("threads", 20, "Threads to use. Default value is 20")
	timeOutPtr := flag.Int("timeout", 5, "Seconds until timeout. Default value is 5")
	querySeperatorPtr := flag.String("queryseperator", "&", "Specify the seperator for queries. The default value is &")
	cacheBusterPtr := flag.String("cachebuster", "cachebuster", "Specify the cachebuster to use. The default value is cachebuster")

	techniqueNames := "cookies,forward,headers,queries,fatget"
	doTestPtr := flag.String("dotest", "", "Choose which tests to run. Use the , seperator to specify multiple ones. Example: -doTest '"+techniqueNames+"'")
	dontTestPtr := flag.String("donttest", "", "Choose which tests to not run. Use the , seperator to specify multiple ones. Example: -dontTest '"+techniqueNames+"'")
	contentTypePtr := flag.String("contenttype", "application/x-www-form-urlencoded", "Set the contenttype for a POST Request. Default is application/x-www-form-urlencoded")
	doPostPtr := flag.Bool("post", false, "Do a POST instead of a GET request.")

	givenCookiesPtr := flag.String("setcookies", "", "Set Cookies. Use the , seperator to specfiy multiple ones")
	givenParametersPtr := flag.String("setqueryparameters", "", "Set Query Parameters. Use --queryseperator (default value is &) as seperator to specfiy multiple ones. E.g.: --setqueryparameters 'user=admin&verbose=true")
	givenHeadersPtr := flag.String("setheaders", "", "Set Headers. Use the , seperator to specfiy multiple ones. Example: -setheaders 'User-Agent: Safari/1.1, Accept-Language: en-US'")
	givenBodyPtr := flag.String("setbody", "", "Set the requests' body")

	headerWordlistPtr := flag.String("headerwordlist", "", "Wordlist for headers to test. Use 'path:' or 'url:'")
	parameterWordlistPtr := flag.String("parameterwordlist", "", "Wordlist for query parameters to test. Use 'path:' or 'url:'")
	topHeadersPtr := flag.Bool("topheaders", false, "Only test for most common headers.")
	topParametersPtr := flag.Bool("topparameters", false, "Only test for most common query parameters.")

	verbosePtr := flag.Bool("verbose", false, "Verbose Output.")
	quietPtr := flag.Bool("quiet", false, "Only show findings.")

	flag.Parse()

	threads = *threadsPtr
	verbose = *verbosePtr
	quiet = *quietPtr
	doPost = *doPostPtr
	contentType = *contentTypePtr
	querySeperator = *querySeperatorPtr
	cacheBuster = *cacheBusterPtr
	/*****************************/

	/* Setting Logoutput to Log file and stdout */
	f, err := os.OpenFile("web-cache-poisoning-scanner.log", os.O_WRONLY|os.O_CREATE|os.O_APPEND, 0644)
	if err != nil {
		log.Fatal(err)
	}
	defer f.Close()
	wrt := io.MultiWriter(f, os.Stdout)
	log.SetOutput(wrt)
	//if !verbose {log.SetOutput(f)}
	/******************************************/

	log.Println("Application started")
	start := time.Now()
	// Making the random generator really random
	rand.Seed(time.Now().UnixNano())

	/* Checking values of Flags */
	if quiet && verbose {
		log.Fatalln("Output can't be both verbose and quiet!")
	}
	if len(flag.Args()) > 0 {
		log.Fatalln(flag.Args(), "Args are not supported! Use flags. Use -h or --help to get a list of all supported flags")
	}
	if *urlPtr == "" && *urlListPtr == "" {
		log.Fatalln("No url specified. Use -url and/or -urllist to specify one or multiple")
	}

	noTestPreference := true
	if *doTestPtr != "" && *dontTestPtr != "" {
		log.Fatalln("You can't set both doTest and dontTest")
	} else if *doTestPtr != "" {
		noTestPreference = false
	} else if *dontTestPtr != "" {
		noTestPreference = false
	}
	/***************************/

	/* Setting up proxy (e.g. burp), if wanted */
	if *proxyCertPathPtr != "" {
		setProxy(*proxyURLPtr, *proxyCertPathPtr)
	}
	/*******************************************/

	/* Reading parameter and header wordlist, if specified */
	var headerList []string
	headerWordlistLower := strings.ToLower(*headerWordlistPtr)
	if strings.HasPrefix(headerWordlistLower, "url:") {
		headersURL = strings.TrimPrefix(headerWordlistLower, "url:")
	} else if strings.HasPrefix(headerWordlistLower, "path:") {
		headerList = GetLocalWordlist(strings.TrimPrefix(headerWordlistLower, "path:"))
	} else if headerWordlistLower != "" {
		log.Println("-headerWordlist has to begin with 'url:' if you use an online wordlist or 'path:' if you use an offline wordlist")
	}

	var parameterList []string
	parameterWordlistLower := strings.ToLower(*parameterWordlistPtr)
	if strings.HasPrefix(parameterWordlistLower, "url:") {
		parametersURL = strings.TrimPrefix(parameterWordlistLower, "url:")
	} else if strings.HasPrefix(parameterWordlistLower, "path:") {
		parameterList = GetLocalWordlist(strings.TrimPrefix(parameterWordlistLower, "path:"))
	} else if parameterWordlistLower != "" {
		log.Println("-parameterWordlist has to begin with 'url:' if you use an online wordlist or 'path:' if you use an offline wordlist")
	}
	/*******************************************************/

	/* Splitting URLs from parameter */
	var urls []string
	if *urlPtr != "" {
		urls = append(urls, *urlPtr)
	}
	if *urlListPtr != "" {
		urls2 := GetLocalWordlist(*urlListPtr)
		urls = append(urls, urls2...)
	}

	givenBody = *givenBodyPtr
	givenCookies = strings.Split(*givenCookiesPtr, ",")
	givenHeaders = strings.Split(*givenHeadersPtr, ",")
	if *givenParametersPtr != "" {
		givenParameters = strings.Split(*givenParametersPtr, querySeperator)
	}

	for i, u := range urls {
		u = strings.TrimSuffix(u, "\r")
		u = strings.TrimSpace(u)
		log.Println("-----------------------------------------------------------------------")
		log.Println("Testing now website(", i+1, "/", len(urls), "):", u)

		/* Setting up client: cookies and noredirect */
		if !quiet {
			log.Println("Setting up client")
		}

		timeout := time.Duration(time.Duration(*timeOutPtr) * time.Second)
		jar, err := cookiejar.New(nil)
		if err != nil {
			log.Fatalln(err)
		}
		jar.SetCookies(website.Url, website.Cookies)

		clientNoRedir := http.Client{
			CheckRedirect: func(redirRequest *http.Request, via []*http.Request) error {
				log.Println("Redirect Request denied:", redirRequest.Header)
				return http.ErrUseLastResponse
			},
			Timeout: timeout,
			Jar:     jar,
		}

		// retrieve cookies
		website = getWebsite(u, clientNoRedir)

		// set cookies
		clientNoRedir.Jar.SetCookies(website.Url, website.Cookies)

		// retrieve response with valid cookies
		website = getWebsite(u, clientNoRedir)
		/*******************************************/

		/* Check cookie for poisoning */
		if noTestPreference || strings.Contains(*doTestPtr, "cookies") || (*dontTestPtr != "" && !strings.Contains(*dontTestPtr, "cookies")) {
			if !quiet {
				log.Println()
				log.Println("Checking cookies for poisoning")
			}
			scanCookies(clientNoRedir)
		} else {
			if !quiet {
				log.Println()
				log.Println("Skipping to check cookies")
			}
		}
		/*****************************/

		/* Check X-Forward-Scheme and X-Forward-Host for poisoning */
		if noTestPreference || strings.Contains(*doTestPtr, "forward") || (*dontTestPtr != "" && !strings.Contains(*dontTestPtr, "forward")) {
			if !quiet {
				log.Println()
				log.Println("Checking for X-Forward-Scheme X-Forward-Host poisoning")
			}
			scanXForwardHeaders(clientNoRedir)
		} else {
			if !quiet {
				log.Println()
				log.Println("Skipping to check forward")
			}
		}
		/***********************************************************/

		/* Checking headers for poisoning */
		if noTestPreference || strings.Contains(*doTestPtr, "headers") || (*dontTestPtr != "" && !strings.Contains(*dontTestPtr, "headers")) {
			if !quiet {
				log.Println()
				log.Println("Testing now headers")
			}
			if len(headerList) == 0 {
				headerList = downloadWordlist("headers", *topHeadersPtr)
			}
			scanHeaders(clientNoRedir, headerList)
		} else {
			if !quiet {
				log.Println()
				log.Println("Skipping to check headers")
			}
		}
		/*********************************/

		/* Checking query parameters for poisoning */
		if noTestPreference || strings.Contains(*doTestPtr, "queries") || (*dontTestPtr != "" && !strings.Contains(*dontTestPtr, "queries")) {
			if !quiet {
				log.Println()
				log.Println("Testing now query parameters")
			}
			if len(parameterList) == 0 {
				parameterList = downloadWordlist("parameters", *topParametersPtr)
			}
			scanParameters(clientNoRedir, parameterList)
		} else {
			if !quiet {
				log.Println()
				log.Println("Skipping to check query parameters")
			}
		}
		/*******************************************/

		/* Trying FAT-GET technique */
		if noTestPreference || strings.Contains(*doTestPtr, "fatget") || (*dontTestPtr != "" && !strings.Contains(*dontTestPtr, "fatget")) {
			if !quiet {
				log.Println()
				log.Println("Testing for FAT-GET")
			}
			//scanFATGET(clientNoRedir)
		} else {
			if !quiet {
				log.Println()
				log.Println("Skipping to check FAT-GET")
			}
		}
		/****************************/
	}

	/* Scan finished */
	log.Println()
	log.Println("Successfully finished the scan")

	duration := time.Since(start)
	log.Println("Duration:", duration)
	/****************/
}

func GetLocalWordlist(pathToWordlist string) []string {
	w, err := ioutil.ReadFile(pathToWordlist)
	if err != nil {
		log.Fatalln(err)
	}
	return strings.Split(string(w), "\n")
}

/* Setting proxy with specified proxyURL and proxyCertPath */
func setProxy(proxyURLString string, proxyCertPath string) {
	proxyURL, err := url.Parse(proxyURLString)
	if err != nil {
		log.Fatal(err)
	}
	caCert, err := ioutil.ReadFile(proxyCertPath)
	if err != nil {
		log.Fatalln(err)
	}
	caCertPool := x509.NewCertPool()
	caCertPool.AppendCertsFromPEM(caCert)

	http.DefaultTransport = &http.Transport{
		Proxy: http.ProxyURL(proxyURL),
		TLSClientConfig: &tls.Config{
			RootCAs: caCertPool,
		}}
}

/* */
func setRequestHeaders(req *http.Request) {
	for _, h := range givenHeaders {
		if h == "" {
			continue
		}

		hSplitted := strings.Split(h, ":")
		if len(hSplitted) != 2 {
			log.Println("The specified header", h, "is not valid. Header key and value need to be seperated by a :")
			continue
		}
		//check if header already exists? use set then instead of add?
		req.Header.Add(strings.TrimSpace(hSplitted[0]), strings.TrimSpace(hSplitted[1]))
	}
}

func setQueryParameterMap(queryParameterMap map[string]string, querySlice []string) map[string]string {
	for _, query := range querySlice {
		query := strings.SplitN(query, "=", 2)
		// ok is true, if a query already is set
		val, ok := queryParameterMap[query[0]]
		if ok {
			log.Println("Overwriting ", query[0], "=", val, " with ", query)
		}
		queryParameterMap[query[0]] = query[1]
	}

	return queryParameterMap
}

func addCacheBuster(strUrl string, cb string) (string, string) {
	if cb == "" {
		cb = randInt()
	}
	strUrl += cacheBuster + "=" + cb

	return strUrl, cb
}

/* Simple get request to get the body of a normal response and the cookies */
func getWebsite(requrl string, client http.Client) Website {

	queryParameterMap := make(map[string]string)

	// splitting url like {https://www.m10x.de/}?{name=max&role=admin}
	urlSlice := strings.SplitN(requrl, "?", 2)

	// splitting queries like {name=max}&{role=admin}
	var querySlice []string
	if strings.Contains(requrl, "?") {
		querySlice = strings.Split(urlSlice[1], querySeperator)
	}

	if len(querySlice) > 0 {
		queryParameterMap = setQueryParameterMap(queryParameterMap, querySlice)
	}

	if len(givenParameters) > 0 {
		queryParameterMap = setQueryParameterMap(queryParameterMap, givenParameters)
	}

	requrl = urlSlice[0] + "?"
	urlNoQueries := urlSlice[0]

	// adding query parameter
	for key, val := range queryParameterMap {
		if !strings.HasSuffix(requrl, "?") {
			requrl += "&"
		}
		requrl += key + "=" + val
	}

	if len(queryParameterMap) > 0 {
		requrl += querySeperator
	}

	// adding cachebuster
	requrlcb, _ := addCacheBuster(requrl, "")

	var req *http.Request
	var err error
	if doPost {
		req, err = http.NewRequest("POST", requrlcb, bytes.NewBufferString(givenBody))
		req.Header.Add("Content-Type", contentType)
	} else {
		req, err = http.NewRequest("GET", requrlcb, nil)
	}
	if err != nil {
		log.Fatalln(err)
	}

	setRequestHeaders(req)
	resp, err := client.Do(req)
	if err != nil {
		log.Fatalln(err)
	}

	defer resp.Body.Close()

	body, err := ioutil.ReadAll(resp.Body)
	if err != nil {
		log.Fatalln(err)
	}

	weburl, err := url.Parse(requrl)
	if err != nil {
		log.Fatalln(err)
	}

	c := Website{
		Body:       string(body),
		Cookies:    resp.Cookies(),
		Status:     resp.Status,
		Url:        weburl,
		BaseUrlStr: urlNoQueries,
		Queries:    queryParameterMap,
	}

	return c
}

/* Create a random long integer */
func randInt() string {
	min := 100000000
	max := 999999999
	result := min + rand.Intn(max-min)
	return strconv.Itoa(result)
}

/* Scan cookies for poisoning */
func scanCookies(client http.Client) {
	for i := 0; i < len(website.Cookies); i++ {
		poison := randInt()
		log.Println("Checking cookie", website.Cookies[i].Name)

		oldValue := website.Cookies[i].Value
		website.Cookies[i].Value = poison
		client.Jar.SetCookies(website.Url, website.Cookies)

		urlCb, cb := addCacheBuster(website.Url.String(), "")

		var req *http.Request
		var err error
		if doPost {
			req, err = http.NewRequest("POST", urlCb, bytes.NewBufferString(givenBody))
			//vorher überprüfen ob bei setRequestHeaders(req) Content-Type schon gesetzt wird? Was präferieren?
			req.Header.Add("Content-Type", contentType)
		} else {
			req, err = http.NewRequest("GET", urlCb, nil)
		}
		if err != nil {
			log.Fatalln(err)
		}

		setRequestHeaders(req)
		resp, err := client.Do(req)
		if err != nil {
			log.Fatalln(err)
		}

		defer resp.Body.Close()

		bodyPoison, err := ioutil.ReadAll(resp.Body)
		if err != nil {
			log.Fatalln(err)
		}

		website.Cookies[i].Value = oldValue
		client.Jar.SetCookies(website.Url, website.Cookies)

		//TODO: Compare (at first) ContentLength instead of whole body?
		if string(bodyPoison) == website.Body {
			continue
		}

		if doPost {
			req, err = http.NewRequest("POST", urlCb, bytes.NewBufferString(givenBody))
			req.Header.Add("Content-Type", contentType)
		} else {
			req, err = http.NewRequest("GET", urlCb, nil)
		}
		if err != nil {
			log.Fatalln(err)
		}

		setRequestHeaders(req)
		resp, err = client.Do(req)
		if err != nil {
			log.Fatalln(err)
		}

		defer resp.Body.Close()

		bodyVictim, err := ioutil.ReadAll(resp.Body)
		if err != nil {
			log.Fatalln(err)
		}

		if strings.Contains(string(bodyVictim), poison) {
			log.Println("")
			log.Println("------- Cookie", website.Cookies[i], "was successfully poisoned!!! cb:", cb, "poison:", poison, "-------")
		}
	}
}

/* Scan X-Forward headers for poisoning */
func scanXForwardHeaders(client http.Client) {
	poison := randInt()

	urlCb, cb := addCacheBuster(website.Url.String(), "")

	var req *http.Request
	var err error
	if doPost {
		req, err = http.NewRequest("POST", urlCb, bytes.NewBufferString(givenBody))
		req.Header.Add("Content-Type", contentType)
	} else {
		req, err = http.NewRequest("GET", urlCb, nil)
	}
	if err != nil {
		log.Fatalln(err)
	}

	setRequestHeaders(req)

	if h := req.Header.Get("X-Forwarded-Host"); h != "" {
		log.Println("Overwriting X-Forwarded-Host" + ":" + h + " with X-Forwarded-Host:" + poison)
		req.Header.Set("X-Forwarded-Host", poison)
	} else {
		req.Header.Add("X-Forwarded-Host", poison)
	}
	if h := req.Header.Get("X-Forwarded-Scheme"); h != "" {
		log.Println("Overwriting X-Forwarded-Scheme" + ":" + h + " with X-Forwarded-Scheme:nothttps")
		req.Header.Set("X-Forwarded-Scheme", "nothttps")
	} else {
		req.Header.Add("X-Forwarded-Scheme", "nothttps")
	}

	resp, err := client.Do(req)
	if err != nil {
		log.Fatalln(err)
	}

	//TODO: Check first request, if second is necessary?
	if doPost {
		req, err = http.NewRequest("POST", urlCb, bytes.NewBufferString(givenBody))
		req.Header.Add("Content-Type", contentType)
	} else {
		req, err = http.NewRequest("GET", urlCb, nil)
	}
	if err != nil {
		log.Fatalln(err)
	}

	setRequestHeaders(req)
	resp, err = client.Do(req)
	if err != nil {
		log.Fatalln(err)
	}
	if strings.Contains(resp.Header.Get("Location"), poison) || strings.Contains(req.Host, poison) {
		log.Println("")
		log.Println("------- X-Forwarded-Host and X-Forwarded-Scheme was successfully poisoned!!! cb:", cb, "poison:", poison, "-------")
	}
}

/* Download a list and split it */
func downloadWordlist(listtype string, top bool) []string {
	var wordlistURL string
	if listtype == "headers" {
		if !quiet {
			log.Println("Downloading header wordlist")
		}
		if top {
			wordlistURL = topHeadersURL
		} else {
			wordlistURL = headersURL
		}
	} else if listtype == "parameters" {
		if !quiet {
			log.Println("Downloading query parameter wordlist")
		}
		if top {
			wordlistURL = topParametersURL
		} else {
			wordlistURL = parametersURL
		}
	} else {
		log.Fatalln("'" + listtype + "' is not a valid parameter for the method downloadWordlist")
	}

	resp, err := http.Get(wordlistURL)
	if err != nil {
		log.Fatalln(err)
	}

	defer resp.Body.Close()

	body, err := ioutil.ReadAll(resp.Body)
	if err != nil {
		log.Fatalln(err)
	}

	return strings.Split(string(body), "\n")
}

/* Scan headers for poisoning */
func scanHeaders(client http.Client, headerList []string) {
	//c := make(chan result) //<- needed?
	var wg sync.WaitGroup

	for i, s := range headerList {
		if s == "" {
			continue
		}

		// wait if max thread count is reached
		if (i+1)%threads == 0 {
			wg.Wait()
		}
		wg.Add(1)

		poison := randInt()

		go func(i int, s string, poison string) {
			defer wg.Done()

			s = strings.Trim(s, "\r")

			if verbose {
				log.Println("Testing now", i, s)
			}

			urlCb, cb := addCacheBuster(website.Url.String(), "")
			var req *http.Request
			var err error
			if doPost {
				req, err = http.NewRequest("POST", urlCb, bytes.NewBufferString(givenBody))
				if strings.ToLower(s) != "content-type" {
					req.Header.Add("Content-Type", contentType)
				}
			} else {
				req, err = http.NewRequest("GET", urlCb, nil)
			}
			if err != nil {
				log.Fatalln(err)
			}

			setRequestHeaders(req)

			if h := req.Header.Get(s); h != "" {
				log.Println("Overwriting " + s + ":" + h + " with " + s + ":" + poison)
				req.Header.Set(s, poison)
			} else {
				req.Header.Add(s, poison)
			}

			resp, err := client.Do(req)
			if err != nil {
				log.Println(err)
				req.Header.Del(s)
				return
			}

			req.Header.Del(s)

			defer resp.Body.Close()
			bodyPoison, err := ioutil.ReadAll(resp.Body)
			if err != nil {
				log.Fatalln(err)
			}

			if string(bodyPoison) == website.Body {
				return
			}

			if doPost {
				req, err = http.NewRequest("POST", urlCb, bytes.NewBufferString(givenBody))
				req.Header.Add("Content-Type", contentType)
			} else {
				req, err = http.NewRequest("GET", urlCb, nil)
			}
			if err != nil {
				log.Fatalln(err)
			}

			setRequestHeaders(req)
			resp, err = client.Do(req)
			if err != nil {
				log.Fatalln(err)
			}

			defer resp.Body.Close()
			bodyVictim, err := ioutil.ReadAll(resp.Body)
			if err != nil {
				log.Fatalln(err)
			}

			if strings.Contains(string(bodyVictim), poison) {
				log.Println("")
				log.Println("------- Header", s, "was successfully poisoned!!! cb:", cb, "poison:", poison, "-------")
			}
		}(i, s, poison)

	}
	wg.Wait()
}

/* Scan query parameters for poisoning */
func scanParameters(client http.Client, parameterList []string) {
	//c := make(chan result) //<- needed?
	var wg sync.WaitGroup

	for i, s := range parameterList {
		if s == "" {
			continue
		}

		// wait if max thread count is reached
		if (i+1)%threads == 0 {
			wg.Wait()
		}
		wg.Add(1)

		poison := randInt()

		go func(i int, s string, poison string) {
			defer wg.Done()

			s = strings.Trim(s, "\r")

			if verbose {
				log.Println("Testing now", i, s)
			}

			var urlCb, cb string
			if _, ok := website.Queries[s]; ok {
				// if the query to add is already present
				queryParameterMap := make(map[string]string)

				for key, val := range website.Queries {
					queryParameterMap[key] = val
				}

				log.Println("Overwriting ", s, "=", queryParameterMap[s], " with ", s, "=", poison)
				queryParameterMap[s] = poison

				urlCb = website.BaseUrlStr + "?"
				for key, val := range queryParameterMap {
					if !strings.HasSuffix(urlCb, "?") {
						urlCb += "&"
					}
					urlCb += key + "=" + val
				}

				urlCb, cb = addCacheBuster(urlCb+querySeperator, "")
			} else {
				// if query isn't already present, just add it and the cachebuster
				urlCb = website.Url.String()
				urlCb += s + "=" + poison + querySeperator
				urlCb, cb = addCacheBuster(urlCb, "")
			}

			var req *http.Request
			var err error
			if doPost {
				req, err = http.NewRequest("POST", urlCb, bytes.NewBufferString(givenBody))
				req.Header.Add("Content-Type", contentType)
			} else {
				req, err = http.NewRequest("GET", urlCb, nil)
			}
			if err != nil {
				log.Fatalln(err)
			}

			setRequestHeaders(req)
			resp, err := client.Do(req)
			if err != nil {
				log.Println(err)
				return
			}

			defer resp.Body.Close()
			bodyPoison, err := ioutil.ReadAll(resp.Body)
			if err != nil {
				log.Fatalln(err)
			}

			if string(bodyPoison) == website.Body {
				return
			}

			// get urlCb with the cachebuster but without the poisoned query
			urlCb, cb = addCacheBuster(website.Url.String(), cb)

			if doPost {
				req, err = http.NewRequest("POST", urlCb, bytes.NewBufferString(givenBody))
				req.Header.Add("Content-Type", contentType)
			} else {
				req, err = http.NewRequest("GET", urlCb, nil)
			}
			if err != nil {
				log.Fatalln(err)
			}

			setRequestHeaders(req)
			resp, err = client.Do(req)
			if err != nil {
				log.Fatalln(err)
			}

			defer resp.Body.Close()
			bodyVictim, err := ioutil.ReadAll(resp.Body)
			if err != nil {
				log.Fatalln(err)
			}

			if strings.Contains(string(bodyVictim), poison) {
				log.Println("")
				log.Println("------- Query Parameter", s, "was successfully poisoned!!! cb:", cb, "poison:", poison, "-------")
			}
		}(i, s, poison)

	}
	wg.Wait()
}

/* Check for FAT-GET */
func scanFATGET(client http.Client) {
	var wg sync.WaitGroup

	wg.Add(1)
	poison := randInt()
	go func(poison string) {
		defer wg.Done()

		if verbose {
			log.Println("Testing now simple FAT-GET")
		}

		urlCb, cb := addCacheBuster(website.Url.String(), "")

		var req *http.Request
		var err error
		if doPost {
			req, err = http.NewRequest("POST", urlCb, bytes.NewBufferString(givenBody))
			req.Header.Add("Content-Type", contentType)
		} else {
			req, err = http.NewRequest("GET", urlCb, nil)
		}
		if err != nil {
			log.Fatalln(err)
		}

		setRequestHeaders(req)
		resp, err := client.Do(req)
		if err != nil {
			log.Println(err)
			return
		}

		defer resp.Body.Close()
		bodyPoison, err := ioutil.ReadAll(resp.Body)
		if err != nil {
			log.Fatalln(err)
		}

		if string(bodyPoison) == website.Body {
			return
		}

		if doPost {
			req, err = http.NewRequest("POST", urlCb, bytes.NewBufferString(givenBody))
			req.Header.Add("Content-Type", contentType)
		} else {
			req, err = http.NewRequest("GET", urlCb, nil)
		}
		if err != nil {
			log.Fatalln(err)
		}

		setRequestHeaders(req)
		resp, err = client.Do(req)
		if err != nil {
			log.Fatalln(err)
		}

		defer resp.Body.Close()
		bodyVictim, err := ioutil.ReadAll(resp.Body)
		if err != nil {
			log.Fatalln(err)
		}

		if strings.Contains(string(bodyVictim), poison) {
			log.Println("")
			log.Println("------- FAT-GET was successfully poisoned!!! cb:", cb, "poison:", poison, "-------")
		}
	}(poison)
}
