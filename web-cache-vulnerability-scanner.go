package main

import (
	"bytes"
	"crypto/tls"
	"crypto/x509"
	"flag"
	"fmt"
	"io"
	"io/ioutil"
	"log"
	"math/rand"
	"net/http"
	"net/http/cookiejar"
	"net/url"
	"os"
	"strconv"
	"strings"
	"sync"
	"text/tabwriter"
	"time"
)

/*
Build module
env GOOS=windows GOARCH=amd64 go build
go build
set GOOS=
set GOARCH=
go build
*/
const version = "1.0.0"

var threads int
var recursivity int
var verbosity int
var doPost bool
var contentType string
var querySeperator string
var cacheBuster string
var statusCode int
var timeOut int

var website Website
var givenBody string
var givenCookies []string
var givenHeaders []string
var givenParameters []string
var impactfulQueries []string

var headersURL string
var parametersURL string
var topHeadersURL string
var topParametersURL string

var generalOptions []FlagStruct
var requestOptions []FlagStruct
var wordlistOptions []FlagStruct

type Website struct {
	Body       string
	Cookies    []*http.Cookie
	Status     string
	Url        *url.URL
	BaseUrlStr string
	Queries    map[string]string
	StatusCode int
}

type FlagStruct struct {
	LongFlag    string
	ShortFlag   string
	Description string
}

func parseFlags() {

}

func help() {
	w := new(tabwriter.Writer)
	w.Init(os.Stdout, 8, 8, 0, '\t', 0)

	fmt.Println("https://github.com/Hackmanit/Web-Cache-Vulnerability-Scanner")
	fmt.Printf("version %s\n\n", version)
	fmt.Print("Usage: Web-Cache-Vulnerability-Scanner(.exe) [options]\n\n")

	fmt.Println("General Options:")
	fmt.Fprintf(w, "%s\t%s\t%s\n", "--help", "-h", "Show this help")
	for _, ts := range generalOptions {
		fmt.Fprintf(w, "--%s\t-%s\t%s\n", ts.LongFlag, ts.ShortFlag, ts.Description)
	}
	w.Flush()

	fmt.Println("\nRequest Options:")
	for _, ts := range requestOptions {
		fmt.Fprintf(w, "--%s\t-%s\t%s\n", ts.LongFlag, ts.ShortFlag, ts.Description)
	}
	w.Flush()

	fmt.Println("\nWordlist Options:")
	for _, ts := range wordlistOptions {
		fmt.Fprintf(w, "--%s\t  -%s\t%s\n", ts.LongFlag, ts.ShortFlag, ts.Description)
	}
	w.Flush()

	os.Exit(0)
}

func appendString(options []FlagStruct, varString *string, longFlag string, shortFlag string, defaultValue string, description string) []FlagStruct {
	flag.StringVar(varString, longFlag, defaultValue, "")
	if shortFlag != longFlag {
		flag.StringVar(varString, shortFlag, defaultValue, "")
	}
	return append(options, FlagStruct{
		LongFlag:    longFlag,
		ShortFlag:   shortFlag,
		Description: description})
}

func appendInt(options []FlagStruct, varInt *int, longFlag string, shortFlag string, defaultValue int, description string) []FlagStruct {
	flag.IntVar(varInt, longFlag, defaultValue, "")
	if shortFlag != longFlag {
		flag.IntVar(varInt, shortFlag, defaultValue, "")
	}
	return append(options, FlagStruct{
		LongFlag:    longFlag,
		ShortFlag:   shortFlag,
		Description: description})
}

func appendBoolean(options []FlagStruct, varBoolean *bool, longFlag string, shortFlag string, defaultValue bool, description string) []FlagStruct {
	flag.BoolVar(varBoolean, longFlag, defaultValue, "")
	if shortFlag != longFlag {
		flag.BoolVar(varBoolean, shortFlag, defaultValue, "")
	}
	return append(options, FlagStruct{
		LongFlag:    longFlag,
		ShortFlag:   shortFlag,
		Description: description})
}

func main() {
	/* Hier die config datei einlesen */
	/* Hier dann die config aus den flags einlesen */

	/* Getting Command-line flags */

	// General Options
	var doTest string
	var dontTest string
	techniqueNames := "cookies,forward,headers,parameters,fatget,cloaking"
	var proxyCertPath string
	var proxyURL string

	generalOptions = appendInt(generalOptions, &verbosity,
		"verbosity", "v", 1, "Set verbosity. 0 = quiet, 1 = normal, 2 = verbose")
	generalOptions = appendInt(generalOptions, &threads,
		"threads", "t", 20, "Threads to use. Default value is 20")
	generalOptions = appendInt(generalOptions, &timeOut,
		"timeout", "to", 10, "Seconds until timeout. Default value is 10")
	generalOptions = appendInt(generalOptions, &timeOut,
		"recursivity", "r", 0, "Put linked scripts/stylesheets/webpages at the end of the queue if the host is the same. Specify how deep the recursivity shall go. Default value is 0 (no recursivity)")
	generalOptions = appendString(generalOptions, &doTest,
		"dotest", "dt", "", "Choose which tests to run. Use the , seperator to specify multiple ones. Example: -doTest '"+techniqueNames+"'")
	generalOptions = appendString(generalOptions, &dontTest,
		"donttest", "dnt", "", "Choose which tests to not run. Use the , seperator to specify multiple ones. Example: -dontTest '"+techniqueNames+"'")
	generalOptions = appendString(generalOptions, &proxyCertPath,
		"proxycertpath", "ppath", "", "Path to the cert of the proxy you want to use. The cert has to have the PEM Format. Burp e.g. is in the DER Format. Use the following command to convert it: openssl x509 -inform DER -outform PEM -text -in cacert.der -out certificate.pem")
	generalOptions = appendString(generalOptions, &proxyURL,
		"proxyurl", "purl", "http://127.0.0.1:8080", "Url for the proxy. Default value is http://127.0.0.1:8080")

	// Request Options
	var urlStr string
	var urlListPath string
	var givenCookiesUnsplitted string
	var givenQueriesUnsplitted string
	var givenHeadersUnsplitted string
	var givenBody string

	requestOptions = appendString(requestOptions, &urlStr,
		"url", "u", "", "Url to scan. Use urllist if you want to scan multiple urls")
	requestOptions = appendString(requestOptions, &urlListPath,
		"urllist", "ul", "", "List with urls to scan. One url per line")
	requestOptions = appendString(requestOptions, &cacheBuster,
		"cachebuster", "cb", "cachebuster", "Specify the cachebuster to use. The default value is cachebuster")
	requestOptions = appendString(requestOptions, &givenCookiesUnsplitted,
		"setcookies", "sc", "", "Set Cookies. Use the , seperator to specfiy multiple ones")
	requestOptions = appendString(requestOptions, &givenQueriesUnsplitted,
		"setqueries", "sq", "", "Set Query Parameters. Use --queryseperator (default value is &) as seperator to specfiy multiple ones. E.g.: --setqueryparameters 'user=admin&verbose=true")
	requestOptions = appendString(requestOptions, &querySeperator,
		"queryseperator", "qs", "&", "Specify the seperator for queries. The default value is &")
	requestOptions = appendString(requestOptions, &givenHeadersUnsplitted,
		"setheaders", "sh", "", "Set Headers. Use the , seperator to specfiy multiple ones. Example: -setheaders 'User-Agent: Safari/1.1, Accept-Language: en-US'")
	requestOptions = appendString(requestOptions, &givenBody,
		"setbody", "sb", "", "Set the requests' body")
	requestOptions = appendBoolean(requestOptions, &doPost,
		"post", "post", false, "Do a POST request instead of a GET request")
	requestOptions = appendString(requestOptions, &contentType,
		"contenttype", "ct", "application/x-www-form-urlencoded", "Set the contenttype for a POST Request. Default is application/x-www-form-urlencoded")
	requestOptions = appendInt(requestOptions, &statusCode,
		"statuscode", "status", 0, "Expected status code of the responses. If not specified it takes the status code of the first response")

	// Wordlist Options
	var headerWordlist string
	var queryWordlist string

	wordlistOptions = appendString(wordlistOptions, &headerWordlist,
		"headerwordlist", "hw", "wordlists/top-headers", "Wordlist for headers to test. Default path is 'wordlists/top-headers'")
	wordlistOptions = appendString(wordlistOptions, &queryWordlist,
		"querywordlist", "qw", "wordlists/top-parameters", "Wordlist for query parameters to test. Default path is 'wordlists/top-parameters'")

	flag.CommandLine.Usage = help

	// flags need to be parsed, before they are used
	flag.Parse()

	parseFlags()
	/*****************************/

	/* Setting Logoutput to Log file and stdout */
	f, err := os.OpenFile("web-cache-poisoning-scanner.log", os.O_WRONLY|os.O_CREATE|os.O_APPEND, 0644)
	if err != nil {
		log.Fatal(err)
	}
	defer f.Close()
	wrt := io.MultiWriter(f, os.Stdout)
	log.SetOutput(wrt)
	//if !verbose {log.SetOutput(f)}
	/******************************************/

	log.Println("Application started")
	start := time.Now()
	// Making the random generator really random
	rand.Seed(time.Now().UnixNano())

	/* Checking values of Flags */
	if len(flag.Args()) > 0 {
		log.Fatalln(flag.Args(), "Args are not supported! Use flags. Use -h or --help to get a list of all supported flags")
	}
	if urlStr == "" && urlListPath == "" {
		log.Fatalln("No url specified. Use -url and/or -urllist to specify one or multiple. Use -h or --help to get a list of all supported flags")
	}

	noTestPreference := true
	if doTest != "" && dontTest != "" {
		log.Fatalln("You can't set both doTest and dontTest")
	} else if doTest != "" {
		noTestPreference = false
	} else if dontTest != "" {
		noTestPreference = false
	}
	/***************************/

	/* Setting up proxy (e.g. burp), if wanted */
	if proxyCertPath != "" {
		setProxy(proxyURL, proxyCertPath)
	}
	/*******************************************/

	// Reading header wordlist, only if it is needed
	var headerList []string
	if noTestPreference || strings.Contains(doTest, "header") || (dontTest != "" && !strings.Contains(dontTest, "header")) {
		headerList = GetLocalWordlist(headerWordlist)
	}

	// Reading parameter wordlist, only if it is needed
	var parameterList []string
	if noTestPreference || strings.Contains(doTest, "parameter") || (dontTest != "" && !strings.Contains(dontTest, "parameter")) {
		parameterList = GetLocalWordlist(queryWordlist)
	}
	/*******************************************************/

	/* Splitting URLs from parameter */
	var urls []string
	if urlStr != "" {
		urls = append(urls, urlStr)
	}
	if urlListPath != "" {
		urls2 := GetLocalWordlist(urlListPath)
		urls = append(urls, urls2...)
	}

	givenCookies = strings.Split(givenCookiesUnsplitted, ",")
	givenHeaders = strings.Split(givenHeadersUnsplitted, ",")
	if givenQueriesUnsplitted != "" {
		givenParameters = strings.Split(givenQueriesUnsplitted, querySeperator)
	}

	for i, u := range urls {
		u = strings.TrimSuffix(u, "\r")
		u = strings.TrimSpace(u)
		log.Println("-----------------------------------------------------------------------")
		log.Println("Testing now website(", i+1, "/", len(urls), "):", u)

		/* Setting up client: cookies and noredirect */
		if verbosity >= 1 {
			log.Println("Setting up client")
		}

		timeOutDuration := time.Duration(time.Duration(timeOut) * time.Second)
		jar, err := cookiejar.New(nil)
		if err != nil {
			log.Fatalln(err)
		}
		jar.SetCookies(website.Url, website.Cookies)

		clientNoRedir := http.Client{
			CheckRedirect: func(redirRequest *http.Request, via []*http.Request) error {
				log.Println("Redirect Request denied:", redirRequest.Header)
				return http.ErrUseLastResponse
			},
			Timeout: timeOutDuration,
			Jar:     jar,
		}

		// retrieve cookies
		website = getWebsite(u, clientNoRedir)

		// set cookies
		clientNoRedir.Jar.SetCookies(website.Url, website.Cookies)

		// retrieve response with valid cookies
		website = getWebsite(u, clientNoRedir)

		// check if there's a cache and the cachebuster works
		checkCache(clientNoRedir)
		/*******************************************/

		/* Check cookie for poisoning */
		if noTestPreference || strings.Contains(doTest, "cookie") || (dontTest != "" && !strings.Contains(dontTest, "cookie")) {
			if verbosity >= 1 {
				log.Println()
				log.Println("Checking cookies for poisoning")
			}
			scanCookies(clientNoRedir)
		} else {
			if verbosity >= 1 {
				log.Println()
				log.Println("Skipping to check cookies")
			}
		}
		/*****************************/

		/* Check X-Forwarded-Scheme and X-Forwarded-Host for poisoning */
		if noTestPreference || strings.Contains(doTest, "forward") || (dontTest != "" && !strings.Contains(dontTest, "forward")) {
			if verbosity >= 1 {
				log.Println()
				log.Println("Checking for X-Forwarded-Scheme X-Forwarded-Host poisoning")
			}
			scanXForwardHeaders(clientNoRedir)
		} else {
			if verbosity >= 1 {
				log.Println()
				log.Println("Skipping to check forward")
			}
		}
		/***********************************************************/

		/* Checking headers for poisoning */
		if noTestPreference || strings.Contains(doTest, "header") || (dontTest != "" && !strings.Contains(dontTest, "header")) {
			if verbosity >= 1 {
				log.Println()
				log.Println("Testing now headers")
			}
			scanHeaders(clientNoRedir, headerList)
		} else {
			if verbosity >= 1 {
				log.Println()
				log.Println("Skipping to check headers")
			}
		}
		/*********************************/

		/* Checking query parameters for poisoning */
		if noTestPreference || strings.Contains(doTest, "parameter") || (dontTest != "" && !strings.Contains(dontTest, "parameter")) {
			if verbosity >= 1 {
				log.Println()
				log.Println("Testing now query parameters")
			}
			scanParameters(clientNoRedir, parameterList)
		} else {
			if verbosity >= 1 {
				log.Println()
				log.Println("Skipping to check query parameters")
			}
		}
		/*******************************************/

		/* Trying fat GET technique */
		if doPost {
			if verbosity >= 1 {
				log.Println()
				log.Println("Can't check for Fat GET, because POST was specified")
			}
		} else if noTestPreference || strings.Contains(doTest, "fatget") || (dontTest != "" && !strings.Contains(dontTest, "fatget")) {
			if verbosity >= 1 {
				log.Println()
				log.Println("Testing for Fat GET")
				scanFatGET(clientNoRedir)
			}
		} else {
			if verbosity >= 1 {
				log.Println()
				log.Println("Skipping to check Fat GET")
			}
		}
		/****************************/

		/* Trying Parameter Cloaking technique */
		if doPost {
			if verbosity >= 1 {
				log.Println()
				log.Println("Can't check for Parameter Cloaking, because POST was specified")
			}
		} else if noTestPreference || strings.Contains(doTest, "cloaking") || (dontTest != "" && !strings.Contains(dontTest, "cloaking")) {
			if verbosity >= 1 {
				log.Println()
				log.Println("Testing for Parameter Cloaking")
				scanParameterCloaking(clientNoRedir)
			}
		} else {
			if verbosity >= 1 {
				log.Println()
				log.Println("Skipping to check Parameter Cloaking")
			}
		}
		/***************************************/
	}

	/* Scan finished */
	log.Println()
	log.Println("Successfully finished the scan")

	duration := time.Since(start)
	log.Println("Duration:", duration)
	/****************/
}

func GetLocalWordlist(pathToWordlist string) []string {
	w, err := ioutil.ReadFile(pathToWordlist)
	if err != nil {
		log.Fatalln(err)
	}
	return strings.Split(string(w), "\n")
}

/* Setting proxy with specified proxyURL and proxyCertPath */
func setProxy(proxyURLString string, proxyCertPath string) {
	proxyURL, err := url.Parse(proxyURLString)
	if err != nil {
		log.Fatal(err)
	}
	caCert, err := ioutil.ReadFile(proxyCertPath)
	if err != nil {
		log.Fatalln(err)
	}
	caCertPool := x509.NewCertPool()
	caCertPool.AppendCertsFromPEM(caCert)

	http.DefaultTransport = &http.Transport{
		Proxy: http.ProxyURL(proxyURL),
		TLSClientConfig: &tls.Config{
			RootCAs: caCertPool,
		}}
}

/* */
func setRequestHeaders(req *http.Request) {
	for _, h := range givenHeaders {
		if h == "" {
			continue
		}

		hSplitted := strings.Split(h, ":")
		if len(hSplitted) != 2 {
			log.Println("The specified header", h, "is not valid. Header key and value need to be seperated by a :")
			continue
		}
		//check if header already exists? use set then instead of add?
		req.Header.Add(strings.TrimSpace(hSplitted[0]), strings.TrimSpace(hSplitted[1]))
	}
}

func setQueryParameterMap(queryParameterMap map[string]string, querySlice []string) map[string]string {
	for _, query := range querySlice {
		query := strings.SplitN(query, "=", 2)
		// ok is true, if a query already is set
		val, ok := queryParameterMap[query[0]]
		if ok {
			log.Println("Overwriting ", query[0], "=", val, " with ", query)
		}
		queryParameterMap[query[0]] = query[1]
	}

	return queryParameterMap
}

func addCacheBuster(strUrl string, cb string) (string, string) {
	if cb == "" {
		cb = randInt()
	}
	strUrl += cacheBuster + "=" + cb

	return strUrl, cb
}

/* Simple get request to get the body of a normal response and the cookies */
func getWebsite(requrl string, client http.Client) Website {

	queryParameterMap := make(map[string]string)

	// splitting url like {https://www.m10x.de/}?{name=max&role=admin}
	urlSlice := strings.SplitN(requrl, "?", 2)

	// splitting queries like {name=max}&{role=admin}
	var querySlice []string
	if strings.Contains(requrl, "?") {
		querySlice = strings.Split(urlSlice[1], querySeperator)
	}

	if len(querySlice) > 0 {
		queryParameterMap = setQueryParameterMap(queryParameterMap, querySlice)
	}

	if len(givenParameters) > 0 {
		queryParameterMap = setQueryParameterMap(queryParameterMap, givenParameters)
	}

	requrl = urlSlice[0] + "?"
	urlNoQueries := urlSlice[0]

	// adding query parameter
	for key, val := range queryParameterMap {
		if !strings.HasSuffix(requrl, "?") {
			requrl += "&"
		}
		requrl += key + "=" + val
	}

	if len(queryParameterMap) > 0 {
		requrl += querySeperator
	}

	// adding cachebuster
	requrlcb, _ := addCacheBuster(requrl, "")

	var req *http.Request
	var err error
	if doPost {
		req, err = http.NewRequest("POST", requrlcb, bytes.NewBufferString(givenBody))
		req.Header.Add("Content-Type", contentType)
	} else {
		req, err = http.NewRequest("GET", requrlcb, nil)
	}
	if err != nil {
		log.Fatalln(err)
	}

	setRequestHeaders(req)
	resp, err := client.Do(req)
	if err != nil {
		log.Fatalln(err)
	}

	defer resp.Body.Close()

	body, err := ioutil.ReadAll(resp.Body)
	if err != nil {
		log.Fatalln(err)
	}

	weburl, err := url.Parse(requrl)
	if err != nil {
		log.Fatalln(err)
	}

	tempStatusCode := statusCode
	if tempStatusCode == 0 {
		tempStatusCode = resp.StatusCode

		log.Println("The default Status Code was set to", tempStatusCode)
	}

	c := Website{
		Body:       string(body),
		Cookies:    resp.Cookies(),
		Status:     resp.Status,
		Url:        weburl,
		BaseUrlStr: urlNoQueries,
		Queries:    queryParameterMap,
	}

	return c
}

func checkCache(client http.Client) {
	/*
		Wie kann ich erkennen, dass ein Cache aktiv ist?
		- X-Cache : hit / miss
		- Zeit messen
	*/
	/*
		Was kann ich als Cache Buster benutzen?
		- Query-Parameter (cachebuster)
		- Header (z.B. origin)
	*/
}

/* Create a random long integer */
func randInt() string {
	min := 100000000
	max := 999999999
	result := min + rand.Intn(max-min)
	return strconv.Itoa(result)
}

/* Scan cookies for poisoning */
func scanCookies(client http.Client) {
	for i := 0; i < len(website.Cookies); i++ {
		poison := randInt()
		log.Println("Checking cookie", website.Cookies[i].Name)

		oldValue := website.Cookies[i].Value
		website.Cookies[i].Value = poison
		client.Jar.SetCookies(website.Url, website.Cookies)

		urlCb, cb := addCacheBuster(website.Url.String(), "")

		errorMessage := website.Cookies[i].String() + "=" + website.Cookies[i].Value

		var req *http.Request
		var err error
		if doPost {
			req, err = http.NewRequest("POST", urlCb, bytes.NewBufferString(givenBody))
			//vorher überprüfen ob bei setRequestHeaders(req) Content-Type schon gesetzt wird? Was präferieren?
			req.Header.Add("Content-Type", contentType)
		} else {
			req, err = http.NewRequest("GET", urlCb, nil)
		}
		if err != nil {
			log.Fatalln(errorMessage, err)
		}

		setRequestHeaders(req)
		resp, err := client.Do(req)
		if err != nil {
			log.Fatalln(errorMessage, err)
		}
		defer resp.Body.Close()

		if resp.StatusCode != website.StatusCode {
			log.Println(resp.StatusCode, website.Cookies[i], "=", website.Cookies[i].Value)
		}

		bodyPoison, err := ioutil.ReadAll(resp.Body)
		if err != nil {
			log.Fatalln(errorMessage, err)
		}

		website.Cookies[i].Value = oldValue
		client.Jar.SetCookies(website.Url, website.Cookies)

		//TODO: Compare (at first) ContentLength instead of whole body?
		if string(bodyPoison) == website.Body {
			continue
		}

		if doPost {
			req, err = http.NewRequest("POST", urlCb, bytes.NewBufferString(givenBody))
			req.Header.Add("Content-Type", contentType)
		} else {
			req, err = http.NewRequest("GET", urlCb, nil)
		}
		if err != nil {
			log.Fatalln(errorMessage, err)
		}

		setRequestHeaders(req)
		resp, err = client.Do(req)
		if err != nil {
			log.Fatalln(errorMessage, err)
		}
		defer resp.Body.Close()

		if resp.StatusCode != website.StatusCode {
			log.Println(resp.StatusCode, website.Cookies[i], "=", website.Cookies[i].Value)
		}

		bodyVictim, err := ioutil.ReadAll(resp.Body)
		if err != nil {
			log.Fatalln(errorMessage, err)
		}

		if strings.Contains(string(bodyVictim), poison) {
			log.Println("")
			log.Println("------- Cookie", website.Cookies[i], "was successfully poisoned!!! cb:", cb, "poison:", poison, "-------")
		}
	}
}

/* Scan X-Forward headers for poisoning */
func scanXForwardHeaders(client http.Client) {
	poison := randInt()

	urlCb, cb := addCacheBuster(website.Url.String(), "")

	var req *http.Request
	var err error
	if doPost {
		req, err = http.NewRequest("POST", urlCb, bytes.NewBufferString(givenBody))
		req.Header.Add("Content-Type", contentType)
	} else {
		req, err = http.NewRequest("GET", urlCb, nil)
	}
	if err != nil {
		log.Fatalln(err)
	}

	setRequestHeaders(req)

	if h := req.Header.Get("X-Forwarded-Host"); h != "" {
		log.Println("Overwriting X-Forwarded-Host" + ":" + h + " with X-Forwarded-Host:" + poison)
		req.Header.Set("X-Forwarded-Host", poison)
	} else {
		req.Header.Add("X-Forwarded-Host", poison)
	}
	if h := req.Header.Get("X-Forwarded-Scheme"); h != "" {
		log.Println("Overwriting X-Forwarded-Scheme" + ":" + h + " with X-Forwarded-Scheme:nothttps")
		req.Header.Set("X-Forwarded-Scheme", "nothttps")
	} else {
		req.Header.Add("X-Forwarded-Scheme", "nothttps")
	}

	resp, err := client.Do(req)
	if err != nil {
		log.Fatalln(err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != website.StatusCode {
		log.Println(resp.StatusCode)
	}

	//TODO: Check first request, if second is necessary?
	if doPost {
		req, err = http.NewRequest("POST", urlCb, bytes.NewBufferString(givenBody))
		req.Header.Add("Content-Type", contentType)
	} else {
		req, err = http.NewRequest("GET", urlCb, nil)
	}
	if err != nil {
		log.Fatalln(err)
	}

	setRequestHeaders(req)
	resp, err = client.Do(req)
	if err != nil {
		log.Fatalln(err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != website.StatusCode {
		log.Println(resp.StatusCode)
	}

	if strings.Contains(resp.Header.Get("Location"), poison) || strings.Contains(req.Host, poison) {
		log.Println("")
		log.Println("------- X-Forwarded-Host and X-Forwarded-Scheme was successfully poisoned!!! cb:", cb, "poison:", poison, "-------")
	}
}

/* Scan headers for poisoning */
func scanHeaders(client http.Client, headerList []string) {
	//c := make(chan result) //<- needed?
	sem := make(chan int, threads)
	var wg sync.WaitGroup
	wg.Add(len(headerList))

	for i, s := range headerList {
		if s == "" {
			if verbosity >= 2 {
				log.Printf("Skipping empty header (%d/%d) %s\n", i+1, len(headerList), s)
			}
			wg.Done()
			continue
		}

		poison := randInt()

		go func(i int, s string, poison string) {
			defer wg.Done()
			sem <- 1

			s = strings.Trim(s, "\r")

			if verbosity >= 2 {
				log.Printf("Testing now (%d/%d) %s\n", i+1, len(headerList), s)
			}

			urlCb, cb := addCacheBuster(website.Url.String(), "")
			var req *http.Request
			var err error
			if doPost {
				req, err = http.NewRequest("POST", urlCb, bytes.NewBufferString(givenBody))
				if strings.ToLower(s) != "content-type" {
					req.Header.Add("Content-Type", contentType)
				}
			} else {
				req, err = http.NewRequest("GET", urlCb, nil)
			}
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			setRequestHeaders(req)

			if h := req.Header.Get(s); h != "" {
				log.Println("Overwriting " + s + ":" + h + " with " + s + ":" + poison)
				req.Header.Set(s, poison)
			} else {
				req.Header.Add(s, poison)
			}

			resp, err := client.Do(req)
			if err != nil {
				<-sem
				log.Println(s, err)
				req.Header.Del(s)
				return
			}
			req.Header.Del(s)

			if resp.StatusCode != website.StatusCode {
				log.Println(resp.StatusCode, s)
			}

			defer resp.Body.Close()
			bodyPoison, err := ioutil.ReadAll(resp.Body)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			if string(bodyPoison) == website.Body {
				<-sem
				return
			}

			if doPost {
				req, err = http.NewRequest("POST", urlCb, bytes.NewBufferString(givenBody))
				req.Header.Add("Content-Type", contentType)
			} else {
				req, err = http.NewRequest("GET", urlCb, nil)
			}
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			setRequestHeaders(req)
			resp, err = client.Do(req)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}
			defer resp.Body.Close()

			if resp.StatusCode != website.StatusCode {
				log.Println(resp.StatusCode, s)
			}

			bodyVictim, err := ioutil.ReadAll(resp.Body)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			if strings.Contains(string(bodyVictim), poison) {
				log.Println("")
				log.Println("------- Header", s, "was successfully poisoned!!! cb:", cb, "poison:", poison, "-------")
			}

			<-sem
		}(i, s, poison)

	}
	wg.Wait()
}

/* Scan query parameters for poisoning */
func scanParameters(client http.Client, parameterList []string) {
	//c := make(chan result) //<- needed?
	sem := make(chan int, threads)
	var wg sync.WaitGroup
	wg.Add(len(parameterList))

	for i, s := range parameterList {
		if s == "" {
			if verbosity >= 2 {
				log.Printf("Skipping empty query (%d/%d) %s\n", i+1, len(parameterList), s)
			}
			wg.Done()
			continue
		}

		poison := randInt()

		go func(i int, s string, poison string) {
			defer wg.Done()
			sem <- 1

			s = strings.Trim(s, "\r")

			if verbosity >= 2 {
				log.Printf("Testing now simple fat GET (%d/%d) %s\n", i+1, len(parameterList), s)
			}

			var urlCb, cb string
			if _, ok := website.Queries[s]; ok {
				// if the query to add is already present
				queryParameterMap := make(map[string]string)

				for key, val := range website.Queries {
					queryParameterMap[key] = val
				}

				log.Println("Overwriting ", s, "=", queryParameterMap[s], " with ", s, "=", poison)
				queryParameterMap[s] = poison

				urlCb = website.BaseUrlStr + "?"
				for key, val := range queryParameterMap {
					if !strings.HasSuffix(urlCb, "?") {
						urlCb += "&"
					}
					urlCb += key + "=" + val
				}

				urlCb, cb = addCacheBuster(urlCb+querySeperator, "")
			} else {
				// if query isn't already present, just add it and the cachebuster
				urlCb = website.Url.String()
				urlCb += s + "=" + poison + querySeperator
				urlCb, cb = addCacheBuster(urlCb, "")
			}

			var req *http.Request
			var err error
			if doPost {
				req, err = http.NewRequest("POST", urlCb, bytes.NewBufferString(givenBody))
				req.Header.Add("Content-Type", contentType)
			} else {
				req, err = http.NewRequest("GET", urlCb, nil)
			}
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			setRequestHeaders(req)
			resp, err := client.Do(req)
			if err != nil {
				<-sem
				log.Println(s, err)
				return
			}
			defer resp.Body.Close()

			if resp.StatusCode != website.StatusCode {
				log.Println(resp.StatusCode, s)
			}

			bodyPoison, err := ioutil.ReadAll(resp.Body)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			// check if something changed
			if string(bodyPoison) == website.Body {
				<-sem
				return
			}

			impactfulQueries = append(impactfulQueries, s)

			// get urlCb with the cachebuster but without the poisoned query
			urlCb, cb = addCacheBuster(website.Url.String(), cb)

			if doPost {
				req, err = http.NewRequest("POST", urlCb, bytes.NewBufferString(givenBody))
				req.Header.Add("Content-Type", contentType)
			} else {
				req, err = http.NewRequest("GET", urlCb, nil)
			}
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			setRequestHeaders(req)
			resp, err = client.Do(req)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}
			defer resp.Body.Close()

			if resp.StatusCode != website.StatusCode {
				log.Println(resp.StatusCode, s)
			}

			bodyVictim, err := ioutil.ReadAll(resp.Body)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			if strings.Contains(string(bodyVictim), poison) {
				log.Println("")
				log.Println("------- Query Parameter", s, "was successfully poisoned!!! cb:", cb, "poison:", poison, "-------")
			}

			<-sem
		}(i, s, poison)

	}
	wg.Wait()
}

/* Check for fat GET */
func scanFatGET(client http.Client) {

	if len(impactfulQueries) == 0 {
		log.Println("No impactful query parameters were found beforehand. Run the query parameter scan (maybe with a different wordlist).")
		return
	}

	//c := make(chan result) //<- needed?
	sem := make(chan int, threads)
	var wg sync.WaitGroup
	wg.Add(len(impactfulQueries))

	for i, s := range impactfulQueries {

		poison := randInt()

		// basic fat get technique
		go func(i int, s string, poison string) {
			defer wg.Done()
			sem <- 1

			if verbosity >= 2 {
				log.Printf("Testing now simple fat GET (%d/%d) %s\n", i+1, len(impactfulQueries), s)
			}

			urlCb, cb := addCacheBuster(website.Url.String(), "")

			var req *http.Request
			var err error

			req, err = http.NewRequest("GET", urlCb, bytes.NewBufferString(s+"="+poison))
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			setRequestHeaders(req)
			resp, err := client.Do(req)
			if err != nil {
				<-sem
				log.Println(s, err)
				return
			}
			defer resp.Body.Close()

			if resp.StatusCode != website.StatusCode {
				log.Println(resp.StatusCode, s)
			}

			body, err := ioutil.ReadAll(resp.Body)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			// check if something changed
			if string(body) == website.Body {
				<-sem
				return
			}

			// get urlCb with the cachebuster but without the poisoned query
			urlCb, cb = addCacheBuster(website.Url.String(), cb)

			req, err = http.NewRequest("GET", urlCb, nil)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			setRequestHeaders(req)
			resp, err = client.Do(req)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}
			defer resp.Body.Close()

			if resp.StatusCode != website.StatusCode {
				log.Println(resp.StatusCode, s)
			}

			body, err = ioutil.ReadAll(resp.Body)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			if strings.Contains(string(body), poison) {
				log.Println("")
				log.Println("------- Query Parameter", s, "was successfully poisoned via simple fat GET!!! cb:", cb, "poison:", poison, "-------")
			}

			<-sem
		}(i, s, poison)
	}
	wg.Wait()
	wg.Add(len(impactfulQueries))
	log.Println()
	for i, s := range impactfulQueries {

		poison := randInt()

		// X-HTTP-Method-Override fat get technique
		go func(i int, s string, poison string) {
			defer wg.Done()
			sem <- 1

			if verbosity >= 2 {
				log.Printf("Testing now X-HTTP-Method-Override fat GET (%d/%d) %s\n", i+1, len(impactfulQueries), s)
			}

			urlCb, cb := addCacheBuster(website.Url.String(), "")

			var req *http.Request
			var err error

			req, err = http.NewRequest("GET", urlCb, bytes.NewBufferString(s+"="+poison))
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			setRequestHeaders(req)
			req.Header.Add("Content-Type", contentType)

			if h := req.Header.Get("X-HTTP-Method-Override"); h != "" {
				log.Println("Overwriting X-HTTP-Method-Override:" + h + " with X-HTTP-Method-Override:" + poison)
				req.Header.Set("X-HTTP-Method-Override", "POST")
			} else {
				req.Header.Add("X-HTTP-Method-Override", "POST")
			}

			resp, err := client.Do(req)
			if err != nil {
				<-sem
				log.Println(s, err)
				return
			}
			defer resp.Body.Close()

			if resp.StatusCode != website.StatusCode {
				log.Println(resp.StatusCode, s)
			}

			body, err := ioutil.ReadAll(resp.Body)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			// check if something changed
			if string(body) == website.Body {
				<-sem
				return
			}

			// get urlCb with the cachebuster but without the poisoned query
			urlCb, cb = addCacheBuster(website.Url.String(), cb)

			req, err = http.NewRequest("GET", urlCb, nil)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			setRequestHeaders(req)
			resp, err = client.Do(req)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}
			defer resp.Body.Close()

			if resp.StatusCode != website.StatusCode {
				log.Println(resp.StatusCode, s)
			}

			body, err = ioutil.ReadAll(resp.Body)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			if strings.Contains(string(body), poison) {
				log.Println("")
				log.Println("------- Query Parameter", s, "was successfully poisoned via X-HTTP-Method-Override fat GET!!! cb:", cb, "poison:", poison, "-------")
			}

			<-sem
		}(i, s, poison)
	}
	wg.Wait()
	wg.Add(len(impactfulQueries))
	log.Println()
	for i, s := range impactfulQueries {

		poison := randInt()

		// basic fat get technique. Also add go func for POST technique and X-Override-HTTP-Method:POST technique
		go func(i int, s string, poison string) {
			defer wg.Done()
			sem <- 1

			if verbosity >= 2 {
				log.Printf("Testing now POST fat GET (%d/%d) %s\n", i+1, len(impactfulQueries), s)
			}

			urlCb, cb := addCacheBuster(website.Url.String(), "")

			var req *http.Request
			var err error

			req, err = http.NewRequest("POST", urlCb, bytes.NewBufferString(s+"="+poison))
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			setRequestHeaders(req)
			req.Header.Add("Content-Type", contentType)

			resp, err := client.Do(req)
			if err != nil {
				<-sem
				log.Println(s, err)
				return
			}
			defer resp.Body.Close()

			if resp.StatusCode != website.StatusCode {
				log.Println(resp.StatusCode, s)
			}

			body, err := ioutil.ReadAll(resp.Body)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			// check if something changed
			if string(body) == website.Body {
				<-sem
				return
			}

			// get urlCb with the cachebuster but without the poisoned query
			urlCb, cb = addCacheBuster(website.Url.String(), cb)

			req, err = http.NewRequest("GET", urlCb, nil)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			setRequestHeaders(req)
			resp, err = client.Do(req)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}
			defer resp.Body.Close()

			if resp.StatusCode != website.StatusCode {
				log.Println(resp.StatusCode, s)
			}

			body, err = ioutil.ReadAll(resp.Body)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			if strings.Contains(string(body), poison) {
				log.Println("")
				log.Println("------- Query Parameter", s, "was successfully poisoned via simple fat GET!!! cb:", cb, "poison:", poison, "-------")
			}

			<-sem
		}(i, s, poison)

	}
	wg.Wait()
}

/* Check for fat GET */
func scanParameterCloaking(client http.Client) {

	if len(impactfulQueries) == 0 {
		log.Println("No impactful query parameters were found beforehand. Run the query parameter scan (maybe with a different wordlist).")
		return
	}

	utm_parameter := []string{"utm_source", "utm_medium", "utm_campaign", "utm_content", "utm_term"}
	unkeyed_parameter := []string{}

	urlCb, _ := addCacheBuster(website.Url.String(), "")

	var req *http.Request
	var err error

	/***********Check if urlCb already contains utm parameter.
				Check if ? or querySeperator is needed
	****************/
	req, err = http.NewRequest("GET", urlCb, nil)
	if err != nil {
		log.Fatalln("first request", err)
	}

	setRequestHeaders(req)
	resp, err := client.Do(req)
	if err != nil {
		log.Println("first request", err)
		return
	}
	defer resp.Body.Close()

	if resp.StatusCode != website.StatusCode {
		log.Println(resp.StatusCode, "first request")
	}

	//c := make(chan result) //<- needed?
	sem := make(chan int, threads)
	var wg sync.WaitGroup
	wg.Add(len(utm_parameter))

	for i, s := range utm_parameter {
		go func(i int, s string) {
			defer wg.Done()
			sem <- 1

			if verbosity >= 2 {
				log.Printf("Testing now for unkeyed query parameters (%d/%d) %s\n", i+1, len(impactfulQueries), s)
			}

			var req *http.Request
			var err error

			req, err = http.NewRequest("GET", urlCb, nil)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			setRequestHeaders(req)
			resp, err := client.Do(req)
			if err != nil {
				<-sem
				log.Println(s, err)
				return
			}
			defer resp.Body.Close()

			/************** if cache hit ************/
			if true {
				unkeyed_parameter = append(unkeyed_parameter, s)
			}

		}(i, s)
	}
	wg.Wait()

	/********************************************************/
	unkeyed_parameter = append(unkeyed_parameter, "utm_content")
	/********************************************************/

	if len(unkeyed_parameter) == 0 {
		log.Println("No unkeyed utm parameters could be found. Parameter Cloaking is not possible using utm parameters")
	} else {
		log.Println("The following utm parameters were found to be unkeyed and will be now testet for parameter cloaking", unkeyed_parameter)
	}

	cloak := ";"
	if querySeperator == ";" {
		cloak = "&"
	}

	wg.Add(len(impactfulQueries))

	for iu, u := range unkeyed_parameter {
		for is, s := range impactfulQueries {

			poison := randInt()

			go func(iu int, u string, is int, s string, poison string) {
				defer wg.Done()
				sem <- 1

				if verbosity >= 2 {
					log.Printf("Testing now Parameter Cloaking (%d/%d) %s%s%s\n", iu+is+1, len(impactfulQueries)*len(unkeyed_parameter), u, cloak, s)
				}

				urlCb, cb := addCacheBuster(website.Url.String(), "")

				var req *http.Request
				var err error

				/***************	add u=test cloak s=poison
				**************/
				req, err = http.NewRequest("GET", urlCb, nil)
				if err != nil {
					<-sem
					log.Fatalln(s, err)
				}

				setRequestHeaders(req)
				resp, err := client.Do(req)
				if err != nil {
					<-sem
					log.Println(s, err)
					return
				}
				defer resp.Body.Close()

				if resp.StatusCode != website.StatusCode {
					log.Println(resp.StatusCode, s)
				}

				body, err := ioutil.ReadAll(resp.Body)
				if err != nil {
					<-sem
					log.Fatalln(s, err)
				}

				// check if something changed
				if string(body) == website.Body {
					<-sem
					return
				}

				// get urlCb with the cachebuster but without the poisoned query
				urlCb, cb = addCacheBuster(website.Url.String(), cb)

				req, err = http.NewRequest("GET", urlCb, nil)
				if err != nil {
					<-sem
					log.Fatalln(s, err)
				}

				setRequestHeaders(req)
				resp, err = client.Do(req)
				if err != nil {
					<-sem
					log.Fatalln(s, err)
				}
				defer resp.Body.Close()

				if resp.StatusCode != website.StatusCode {
					log.Println(resp.StatusCode, s)
				}

				body, err = ioutil.ReadAll(resp.Body)
				if err != nil {
					<-sem
					log.Fatalln(s, err)
				}

				if strings.Contains(string(body), poison) {
					log.Println("")
					log.Println("------- Query Parameter", s, "was successfully poisoned via simple fat GET!!! cb:", cb, "poison:", poison, "-------")
				}

				<-sem
			}(iu, u, is, s, poison)
		}
	}
	wg.Wait()
}
