package main

import (
	"bytes"
	"crypto/tls"
	"crypto/x509"
	"flag"
	"fmt"
	"io"
	"io/ioutil"
	"log"
	"math/rand"
	"net/http"
	"net/url"
	"os"
	"strconv"
	"strings"
	"sync"
	"text/tabwriter"
	"time"
)

/*
Build module
env GOOS=windows GOARCH=amd64 go build
go build
set GOOS=
set GOARCH=
go build
*/
const version = "1.0.0"

var threads int
var recursivity int
var verbosity int
var doPost bool
var contentType string
var querySeperator string
var cacheBuster string
var statusCode int
var timeOut int

var website Website
var impactfulQueries []string

var urls []string
var cookies []string
var headers []string
var parameters []string
var body string

var generalOptions []FlagStruct
var requestOptions []FlagStruct
var wordlistOptions []FlagStruct

type Website struct {
	Body       string
	Cookies    []*http.Cookie
	Url        *url.URL
	BaseUrlStr string
	Queries    map[string]string
	StatusCode int
}

type FlagStruct struct {
	LongFlag    string
	ShortFlag   string
	Description string
}

func parseFlags() {

}

func help() {
	w := new(tabwriter.Writer)
	w.Init(os.Stdout, 8, 8, 0, '\t', 0)

	fmt.Println("https://github.com/Hackmanit/Web-Cache-Vulnerability-Scanner")
	fmt.Printf("version %s\n\n", version)
	fmt.Print("Usage: Web-Cache-Vulnerability-Scanner(.exe) [options]\n\n")

	fmt.Println("General Options:")
	fmt.Fprintf(w, "%s\t%s\t%s\n", "--help", "-h", "Show this help and quit")
	for _, ts := range generalOptions {
		fmt.Fprintf(w, "--%s\t-%s\t%s\n", ts.LongFlag, ts.ShortFlag, ts.Description)
	}
	w.Flush()

	fmt.Println("\nRequest Options:")
	for _, ts := range requestOptions {
		fmt.Fprintf(w, "--%s\t-%s\t%s\n", ts.LongFlag, ts.ShortFlag, ts.Description)
	}
	w.Flush()

	fmt.Println("\nWordlist Options:")
	for _, ts := range wordlistOptions {
		fmt.Fprintf(w, "--%s\t  -%s\t%s\n", ts.LongFlag, ts.ShortFlag, ts.Description)
	}
	w.Flush()

	os.Exit(0)
}

func appendString(options []FlagStruct, varString *string, longFlag string, shortFlag string, defaultValue string, description string) []FlagStruct {
	flag.StringVar(varString, longFlag, defaultValue, "")
	if shortFlag != longFlag {
		flag.StringVar(varString, shortFlag, defaultValue, "")
	}
	return append(options, FlagStruct{
		LongFlag:    longFlag,
		ShortFlag:   shortFlag,
		Description: description})
}

func appendInt(options []FlagStruct, varInt *int, longFlag string, shortFlag string, defaultValue int, description string) []FlagStruct {
	flag.IntVar(varInt, longFlag, defaultValue, "")
	if shortFlag != longFlag {
		flag.IntVar(varInt, shortFlag, defaultValue, "")
	}
	return append(options, FlagStruct{
		LongFlag:    longFlag,
		ShortFlag:   shortFlag,
		Description: description})
}

func appendBoolean(options []FlagStruct, varBoolean *bool, longFlag string, shortFlag string, defaultValue bool, description string) []FlagStruct {
	flag.BoolVar(varBoolean, longFlag, defaultValue, "")
	if shortFlag != longFlag {
		flag.BoolVar(varBoolean, shortFlag, defaultValue, "")
	}
	return append(options, FlagStruct{
		LongFlag:    longFlag,
		ShortFlag:   shortFlag,
		Description: description})
}

func main() {
	/* Hier die config datei einlesen */
	/* Hier dann die config aus den flags einlesen */

	/* Getting Command-line flags */

	// General Options
	var doTest string
	var dontTest string
	techniqueNames := "cookies,forward,headers,parameters,fatget,cloaking"
	var proxyCertPath string
	var proxyURL string

	generalOptions = appendInt(generalOptions, &verbosity,
		"verbosity", "v", 1, "Set verbosity. 0 = quiet, 1 = normal, 2 = verbose")
	generalOptions = appendInt(generalOptions, &threads,
		"threads", "t", 20, "Threads to use. Default value is 20")
	generalOptions = appendInt(generalOptions, &timeOut,
		"timeout", "to", 10, "Seconds until timeout. Default value is 10")
	generalOptions = appendInt(generalOptions, &timeOut,
		"recursivity", "r", 0, "Put linked scripts/stylesheets/webpages at the end of the queue if the host is the same. Specify how deep the recursivity shall go. Default value is 0 (no recursivity)")
	generalOptions = appendString(generalOptions, &doTest,
		"dotest", "dt", "", "Choose which tests to run. Use the , seperator to specify multiple ones. Example: -doTest '"+techniqueNames+"'")
	generalOptions = appendString(generalOptions, &dontTest,
		"donttest", "dnt", "", "Choose which tests to not run. Use the , seperator to specify multiple ones. Example: -dontTest '"+techniqueNames+"'")
	generalOptions = appendString(generalOptions, &proxyCertPath,
		"proxycertpath", "ppath", "", "Path to the cert of the proxy you want to use. The cert has to have the PEM Format. Burp e.g. is in the DER Format. Use the following command to convert it: openssl x509 -inform DER -outform PEM -text -in cacert.der -out certificate.pem")
	generalOptions = appendString(generalOptions, &proxyURL,
		"proxyurl", "purl", "http://127.0.0.1:8080", "Url for the proxy. Default value is http://127.0.0.1:8080")

	// Request Options
	var urlStr string
	var retrieveCookies bool
	var setCookiesStr string
	var setHeadersStr string
	var setParametersStr string
	var setBodyStr string

	requestOptions = appendString(requestOptions, &urlStr,
		"url", "u", "", "Url to scan. Has to start with http:// or https://. Otherwise use file: to specify a file with (multiple) urls. E.g. -u https://www.example.com or -u file:templates/url_list")
	requestOptions = appendBoolean(requestOptions, &retrieveCookies,
		"retrieveCookies", "rc", false, "Do you want to use cookies, received in the response of the first request? Default value is false")
	requestOptions = appendString(requestOptions, &cacheBuster,
		"cachebuster", "cb", "cachebuster", "Specify the cachebuster to use. The default value is cachebuster")
	requestOptions = appendString(requestOptions, &setCookiesStr,
		"setcookies", "sc", "", "Set a Cookie. Otherwise use file: to specify a file with urls. E.g. -sc uid=123 or -sc file:templates/cookie_list")
	requestOptions = appendString(requestOptions, &setHeadersStr,
		"setheaders", "sh", "", "Set a Header. Otherwise use file: to specify a file with urls. E.g. -sh 'User-Agent: Safari/1.1' or -sh file:templates/header_list")
	requestOptions = appendString(requestOptions, &setParametersStr,
		"setparameters", "sp", "", "Set a Query Parameter. Otherwise use file: to specify a file with urls. E.g. -sp user=admin or -sp file:templates/parameter_list")
	requestOptions = appendString(requestOptions, &setBodyStr,
		"setbody", "sb", "", "Set the requests' body. Otherwise use file: to specify a file with urls. E.g. -sb 'admin=true' or -sh file:templates/body_file")
	requestOptions = appendBoolean(requestOptions, &doPost,
		"post", "post", false, "Do a POST request instead of a GET request")
	requestOptions = appendString(requestOptions, &contentType,
		"contenttype", "ct", "application/x-www-form-urlencoded", "Set the contenttype for a POST Request. Default is application/x-www-form-urlencoded. If you don't want a content-type to be used at all use -ct ''")
	requestOptions = appendInt(requestOptions, &statusCode,
		"statuscode", "status", 0, "Expected status code of the responses. If not specified it takes the status code of the first response")
	requestOptions = appendString(requestOptions, &querySeperator,
		"parameterseperator", "ps", "&", "Specify the seperator for parameters. The default value is &")

	// Wordlist Options
	var headerWordlist string
	var queryWordlist string

	wordlistOptions = appendString(wordlistOptions, &headerWordlist,
		"headerwordlist", "hw", "wordlists/top-headers", "Wordlist for headers to test. Default path is 'wordlists/top-headers'")
	wordlistOptions = appendString(wordlistOptions, &queryWordlist,
		"querywordlist", "qw", "wordlists/top-parameters", "Wordlist for query parameters to test. Default path is 'wordlists/top-parameters'")

	flag.CommandLine.Usage = help

	// flags need to be parsed, before they are used
	flag.Parse()

	parseFlags()
	/*****************************/

	/* Setting Logoutput to Log file and stdout */
	f, err := os.OpenFile("web-cache-poisoning-scanner.log", os.O_WRONLY|os.O_CREATE|os.O_APPEND, 0644)
	if err != nil {
		log.Fatal(err)
	}
	defer f.Close()
	wrt := io.MultiWriter(f, os.Stdout)
	log.SetOutput(wrt)
	//if !verbose {log.SetOutput(f)}
	/******************************************/

	log.Println("Application started")
	start := time.Now()
	// Making the random generator really random
	rand.Seed(time.Now().UnixNano())

	/* Checking values of Flags */
	if len(flag.Args()) > 0 {
		log.Fatalln(flag.Args(), "Args are not supported! Use flags. Use -h or --help to get a list of all supported flags")
	}
	if urlStr == "" {
		log.Fatalln("No url specified. Use -url or -u. Use -h or --help to get a list of all supported flags")
	}

	noTestPreference := true
	if doTest != "" && dontTest != "" {
		log.Fatalln("You can't set both doTest and dontTest")
	} else if doTest != "" {
		noTestPreference = false
	} else if dontTest != "" {
		noTestPreference = false
	}
	/***************************/

	/* Setting up proxy (e.g. burp), if wanted */
	if proxyCertPath != "" {
		setProxy(proxyURL, proxyCertPath)
	}
	/*******************************************/

	// Reading header wordlist, only if it is needed
	var headerList []string
	if noTestPreference || strings.Contains(doTest, "header") || (dontTest != "" && !strings.Contains(dontTest, "header")) {
		headerList = ReadLocalFile(headerWordlist)
	}

	// Reading parameter wordlist, only if it is needed
	var parameterList []string
	if noTestPreference || strings.Contains(doTest, "parameter") || (dontTest != "" && !strings.Contains(dontTest, "parameter")) {
		parameterList = ReadLocalFile(queryWordlist)
	}
	/*******************************************************/

	/* Read URL(s) */
	if strings.HasPrefix(urlStr, "path:") {
		urls = ReadLocalFile(urlStr)
	} else {
		urls = append(urls, urlStr)
	}

	/* Read Cookie(s) */
	if strings.HasPrefix(setCookiesStr, "path:") {
		cookies = ReadLocalFile(setCookiesStr)
	} else {
		cookies = append(cookies, setCookiesStr)
	}

	/* Read Header(s) */
	if strings.HasPrefix(setHeadersStr, "path:") {
		headers = ReadLocalFile(setHeadersStr)
	} else {
		headers = append(headers, setHeadersStr)
	}

	/* Read Parameter(s) */
	if strings.HasPrefix(setParametersStr, "path:") {
		parameters = ReadLocalFile(setParametersStr)
	} else {
		parameters = append(parameters, setParametersStr)
	}

	/* Read Body */
	if strings.HasPrefix(setBodyStr, "path:") {
		bodySlice := ReadLocalFile(setBodyStr)
		for _, l := range bodySlice {
			l = strings.TrimSuffix(l, "\r")
			l = strings.TrimSpace(l)
			if strings.HasPrefix(l, "//") || l == "" {
				continue
			}
			body += l
		}
	} else {
		body = setBodyStr
	}

	for i, u := range urls {
		u = strings.TrimSuffix(u, "\r")
		u = strings.TrimSpace(u)

		// check if empty or is a comment
		if u == "" || strings.HasPrefix(u, "//") {
			continue
		}

		log.Println("-----------------------------------------------------------------------")
		log.Printf("Testing now website(%d/%d): %s\n", i+1, len(urls), u)

		/* Setting up client: cookies and noredirect */
		if verbosity >= 1 {
			log.Println("Setting up client")
		}

		// Setting cookies, speficied by setcookies
		website.Cookies = []*http.Cookie{}
		for _, c := range cookies {
			c = strings.TrimSuffix(c, "\r")
			c = strings.TrimSpace(c)
			if c == "" {
				continue
			} else if !strings.Contains(c, "=") {
				log.Printf("Specified cookie %s doesn't contain a = and will be skipped\n", c)
				continue
			} else {
				cSlice := strings.SplitAfterN(c, "=", 2)
				cSlice[0] = strings.TrimSuffix(cSlice[0], "=")

				cookie := http.Cookie{
					Name:  cSlice[0],
					Value: cSlice[1],
				}
				website.Cookies = append(website.Cookies, &cookie)
			}
		}

		timeOutDuration := time.Duration(time.Duration(timeOut) * time.Second)
		clientNoRedir := http.Client{
			CheckRedirect: func(redirRequest *http.Request, via []*http.Request) error {
				log.Printf("Redirect Request denied: %s\n", redirRequest.Header)
				return http.ErrUseLastResponse
			},
			Timeout: timeOutDuration,
		}

		// retrieve cookies
		website = getWebsite(u, clientNoRedir, retrieveCookies, true)

		if retrieveCookies {
			// retrieve response with all cookies set
			website = getWebsite(u, clientNoRedir, retrieveCookies, false)
		}

		// check if there's a cache and the cachebuster works
		checkCache(clientNoRedir)
		/*******************************************/

		/* Check cookie for poisoning */
		if noTestPreference || strings.Contains(doTest, "cookie") || (dontTest != "" && !strings.Contains(dontTest, "cookie")) {
			if verbosity >= 1 {
				log.Println()
				log.Println("Checking cookies for poisoning")
			}
			scanCookies(clientNoRedir)
		} else {
			if verbosity >= 1 {
				log.Println()
				log.Println("Skipping to check cookies")
			}
		}
		/*****************************/

		/* Check X-Forwarded-Scheme and X-Forwarded-Host for poisoning */
		if noTestPreference || strings.Contains(doTest, "forward") || (dontTest != "" && !strings.Contains(dontTest, "forward")) {
			if verbosity >= 1 {
				log.Println()
				log.Println("Checking for X-Forwarded-Scheme X-Forwarded-Host poisoning")
			}
			scanXForwardHeaders(clientNoRedir)
		} else {
			if verbosity >= 1 {
				log.Println()
				log.Println("Skipping to check forward")
			}
		}
		/***********************************************************/

		/* Checking headers for poisoning */
		if noTestPreference || strings.Contains(doTest, "header") || (dontTest != "" && !strings.Contains(dontTest, "header")) {
			if verbosity >= 1 {
				log.Println()
				log.Println("Testing now headers")
			}
			scanHeaders(clientNoRedir, headerList)
		} else {
			if verbosity >= 1 {
				log.Println()
				log.Println("Skipping to check headers")
			}
		}
		/*********************************/

		/* Checking query parameters for poisoning */
		if noTestPreference || strings.Contains(doTest, "parameter") || (dontTest != "" && !strings.Contains(dontTest, "parameter")) {
			if verbosity >= 1 {
				log.Println()
				log.Println("Testing now query parameters")
			}
			scanParameters(clientNoRedir, parameterList)
		} else {
			if verbosity >= 1 {
				log.Println()
				log.Println("Skipping to check query parameters")
			}
		}
		/*******************************************/

		/* Trying fat GET technique */
		if doPost {
			if verbosity >= 1 {
				log.Println()
				log.Println("Can't check for Fat GET, because POST was specified")
			}
		} else if noTestPreference || strings.Contains(doTest, "fatget") || (dontTest != "" && !strings.Contains(dontTest, "fatget")) {
			if verbosity >= 1 {
				log.Println()
				log.Println("Testing for Fat GET")
				scanFatGET(clientNoRedir)
			}
		} else {
			if verbosity >= 1 {
				log.Println()
				log.Println("Skipping to check Fat GET")
			}
		}
		/****************************/

		/* Trying Parameter Cloaking technique */
		if doPost {
			if verbosity >= 1 {
				log.Println()
				log.Println("Can't check for Parameter Cloaking, because POST was specified")
			}
		} else if noTestPreference || strings.Contains(doTest, "cloaking") || (dontTest != "" && !strings.Contains(dontTest, "cloaking")) {
			if verbosity >= 1 {
				log.Println()
				log.Println("Testing for Parameter Cloaking")
				scanParameterCloaking(clientNoRedir)
			}
		} else {
			if verbosity >= 1 {
				log.Println()
				log.Println("Skipping to check Parameter Cloaking")
			}
		}
		/***************************************/
	}

	/* Scan finished */
	log.Println()
	log.Println("Successfully finished the scan")

	duration := time.Since(start)
	log.Printf("Duration: %s", duration)
	/****************/
}

func ReadLocalFile(path string) []string {
	if strings.HasPrefix(path, "path:") {
		strings.TrimPrefix(path, "path:")
	} else if strings.HasPrefix(strings.ToLower(path), "path:") {
		log.Fatalln("Please make sure that path: is lowercase")
	}

	w, err := ioutil.ReadFile(path)
	if err != nil {
		log.Fatalln(err)
	}

	return strings.Split(string(w), "\n")
}

/* Setting proxy with specified proxyURL and proxyCertPath */
func setProxy(proxyURLString string, proxyCertPath string) {
	proxyURL, err := url.Parse(proxyURLString)
	if err != nil {
		log.Fatal(err)
	}
	caCert, err := ioutil.ReadFile(proxyCertPath)
	if err != nil {
		log.Fatalln(err)
	}
	caCertPool := x509.NewCertPool()
	caCertPool.AppendCertsFromPEM(caCert)

	http.DefaultTransport = &http.Transport{
		Proxy: http.ProxyURL(proxyURL),
		TLSClientConfig: &tls.Config{
			RootCAs: caCertPool,
		}}
}

func setRequest(req *http.Request, doPost bool) {
	setRequestHeaders(req)
	setRequestCookies(req)

	// Content-Type nur hinzufügen, wenn nicht schon vorher geschehen
	if doPost {
		if req.Header.Get("Content-Type") == "" && contentType != "" {
			req.Header.Add("Content-Type", contentType)
		}
	}
}

/* TODO wie bei requestCookies nur die erste occurance eines headers aufnehmen */
func setRequestHeaders(req *http.Request) {
	for _, h := range headers {
		h = strings.TrimSuffix(h, "\r")
		h = strings.TrimSpace(h)
		if h == "" {
			continue
		} else if !strings.Contains(h, ":") {
			log.Println("Specified header", h, "doesn't contain a : and will be skipped")
			continue
		} else {
			hSplitted := strings.Split(h, ":")

			req.Header.Add(strings.TrimSpace(hSplitted[0]), strings.TrimSpace(hSplitted[1]))
		}
	}
}

/* */
func setRequestCookies(req *http.Request) {
	for i, c := range website.Cookies {
		// only add first occurence of a cookie to the request
		// remove every other occurence
		_, err := req.Cookie(c.Name)
		if err == http.ErrNoCookie {
			req.AddCookie(c)
		} else if i != len(website.Cookies) {
			website.Cookies = append(website.Cookies[:i], website.Cookies[i+1:]...)
		}
	}
}

func setQueryParameterMap(queryParameterMap map[string]string, querySlice []string) map[string]string {
	for _, q := range querySlice {
		q = strings.TrimSuffix(q, "\r")
		q = strings.TrimSpace(q)
		if q == "" {
			continue
		} else if !strings.Contains(q, "=") {
			log.Printf("Specified parameter %s doesn't contain a = and will be skipped\n", q)
			continue
		} else {
			query := strings.SplitN(q, "=", 2)
			// ok is true, if a query already is set
			val, ok := queryParameterMap[query[0]]
			if ok {
				log.Printf("Overwriting %s=%s with %s=%s\n", query[0], val, query[0], query[1])
			}
			queryParameterMap[query[0]] = query[1]
		}
	}

	return queryParameterMap
}

func addCacheBuster(strUrl string, cb string) (string, string) {
	if cb == "" {
		cb = randInt()
	}
	strUrl += cacheBuster + "=" + cb

	return strUrl, cb
}

/* Simple get request to get the body of a normal response and the cookies */
func getWebsite(requrl string, client http.Client, retrieveCookies bool, firstRequest bool) Website {

	queryParameterMap := make(map[string]string)

	// splitting url like {https://www.m10x.de/}?{name=max&role=admin}
	urlSlice := strings.SplitN(requrl, "?", 2)

	// splitting queries like {name=max}&{role=admin}
	var parameterSlice []string
	if strings.Contains(requrl, "?") {
		parameterSlice = strings.Split(urlSlice[1], querySeperator)
	}

	if len(parameterSlice) > 0 {
		queryParameterMap = setQueryParameterMap(queryParameterMap, parameterSlice)
	}

	if len(parameters) > 0 {
		queryParameterMap = setQueryParameterMap(queryParameterMap, parameters)
	}

	requrl = urlSlice[0] + "?"
	urlNoQueries := urlSlice[0]

	// adding query parameter
	for key, val := range queryParameterMap {
		if !strings.HasSuffix(requrl, "?") {
			requrl += "&"
		}
		requrl += key + "=" + val
	}

	if len(queryParameterMap) > 0 {
		requrl += querySeperator
	}

	// adding cachebuster
	requrlcb, _ := addCacheBuster(requrl, "")

	var req *http.Request
	var err error
	if doPost {
		req, err = http.NewRequest("POST", requrlcb, bytes.NewBufferString(body))
	} else {
		req, err = http.NewRequest("GET", requrlcb, nil)
	}
	if err != nil {
		log.Fatalln(err)
	}

	setRequest(req, doPost)
	resp, err := client.Do(req)
	if err != nil {
		log.Fatalln(err)
	}

	defer resp.Body.Close()

	body, err := ioutil.ReadAll(resp.Body)
	if err != nil {
		log.Fatalln(err)
	}

	weburl, err := url.Parse(requrl)
	if err != nil {
		log.Fatalln(err)
	}

	tempStatusCode := statusCode
	if tempStatusCode == 0 {
		tempStatusCode = resp.StatusCode

		log.Printf("The default Status Code was set to %d\n", tempStatusCode)
	}

	// if retrieveCookies is false, only the specified cookies will be used
	// otherwise the by the server given cookies AND the specified cookies will be used
	cookiesWebsite := website.Cookies
	if retrieveCookies {
		cookiesWebsite = append(cookiesWebsite, resp.Cookies()...)
	}

	c := Website{
		Body:       string(body),
		Cookies:    cookiesWebsite,
		StatusCode: tempStatusCode,
		Url:        weburl,
		BaseUrlStr: urlNoQueries,
		Queries:    queryParameterMap,
	}

	return c
}

func checkCache(client http.Client) {
	/*
		Wie kann ich erkennen, dass ein Cache aktiv ist?
		- X-Cache : hit / miss
		- Zeit messen
	*/
	/*
		Was kann ich als Cache Buster benutzen?
		- Query-Parameter (cachebuster)
		- Header (z.B. origin)
	*/
}

/* Create a random long integer */
func randInt() string {
	min := 100000000
	max := 999999999
	result := min + rand.Intn(max-min)
	return strconv.Itoa(result)
}

/* Scan cookies for poisoning */
func scanCookies(client http.Client) {
	for i := 0; i < len(website.Cookies); i++ {
		poison := randInt()
		log.Println("Checking cookie", website.Cookies[i].Name)

		urlCb, cb := addCacheBuster(website.Url.String(), "")

		errorMessage := website.Cookies[i].String() + "=" + website.Cookies[i].Value

		var req *http.Request
		var err error
		if doPost {
			req, err = http.NewRequest("POST", urlCb, bytes.NewBufferString(body))
		} else {
			req, err = http.NewRequest("GET", urlCb, nil)
		}
		if err != nil {
			log.Fatalln(errorMessage, err)
		}

		log.Printf("Overwriting %s=%s with %s=%s\n", website.Cookies[i].Name, website.Cookies[i].Value, website.Cookies[i].Name, poison)
		oldValue := website.Cookies[i].Value
		website.Cookies[i].Value = poison

		setRequest(req, doPost)

		resp, err := client.Do(req)
		if err != nil {
			log.Fatalln(errorMessage, err)
		}
		defer resp.Body.Close()

		if resp.StatusCode != website.StatusCode {
			log.Printf("Unexpected Status Code %d for %s=%s\n", resp.StatusCode, website.Cookies[i], website.Cookies[i].Value)
		}

		bodyPoison, err := ioutil.ReadAll(resp.Body)
		if err != nil {
			log.Fatalln(errorMessage, err)
		}

		website.Cookies[i].Value = oldValue

		//TODO: Compare (at first) ContentLength instead of whole body?
		if string(bodyPoison) == website.Body {
			continue
		}

		if doPost {
			req, err = http.NewRequest("POST", urlCb, bytes.NewBufferString(body))
		} else {
			req, err = http.NewRequest("GET", urlCb, nil)
		}
		if err != nil {
			log.Fatalln(errorMessage, err)
		}

		setRequest(req, doPost)
		resp, err = client.Do(req)
		if err != nil {
			log.Fatalln(errorMessage, err)
		}
		defer resp.Body.Close()

		if resp.StatusCode != website.StatusCode {
			log.Printf("Unexpected Status Code %d for %s=%s\n", resp.StatusCode, website.Cookies[i], website.Cookies[i].Value)
		}

		bodyVictim, err := ioutil.ReadAll(resp.Body)
		if err != nil {
			log.Fatalln(errorMessage, err)
		}

		if strings.Contains(string(bodyVictim), poison) {
			log.Println("")
			log.Printf("------- Cookie %s was successfully poisoned!!! cb: %s poison: %s -------\n", website.Cookies[i].Name, cb, poison)
		}
	}
}

/* Scan X-Forward headers for poisoning */
func scanXForwardHeaders(client http.Client) {
	poison := randInt()

	urlCb, cb := addCacheBuster(website.Url.String(), "")

	var req *http.Request
	var err error
	if doPost {
		req, err = http.NewRequest("POST", urlCb, bytes.NewBufferString(body))
	} else {
		req, err = http.NewRequest("GET", urlCb, nil)
	}
	if err != nil {
		log.Fatalln(err)
	}

	setRequest(req, doPost)

	if h := req.Header.Get("X-Forwarded-Host"); h != "" {
		log.Println("Overwriting X-Forwarded-Host" + ":" + h + " with X-Forwarded-Host:" + poison)
		req.Header.Set("X-Forwarded-Host", poison)
	} else {
		req.Header.Add("X-Forwarded-Host", poison)
	}
	if h := req.Header.Get("X-Forwarded-Scheme"); h != "" {
		log.Println("Overwriting X-Forwarded-Scheme" + ":" + h + " with X-Forwarded-Scheme:nothttps")
		req.Header.Set("X-Forwarded-Scheme", "nothttps")
	} else {
		req.Header.Add("X-Forwarded-Scheme", "nothttps")
	}

	resp, err := client.Do(req)
	if err != nil {
		log.Fatalln(err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != website.StatusCode {
		log.Printf("Unexpected Status Code %d\n", resp.StatusCode)
	}

	//TODO: Check first request, if second is necessary?
	if doPost {
		req, err = http.NewRequest("POST", urlCb, bytes.NewBufferString(body))
	} else {
		req, err = http.NewRequest("GET", urlCb, nil)
	}
	if err != nil {
		log.Fatalln(err)
	}

	setRequest(req, doPost)
	resp, err = client.Do(req)
	if err != nil {
		log.Fatalln(err)
	}
	defer resp.Body.Close()

	if resp.StatusCode != website.StatusCode {
		log.Printf("Unexpected Status Code %d\n", resp.StatusCode)
	}

	if strings.Contains(resp.Header.Get("Location"), poison) || strings.Contains(req.Host, poison) {
		log.Println("")
		log.Println("------- X-Forwarded-Host and X-Forwarded-Scheme was successfully poisoned!!! cb:", cb, "poison:", poison, "-------")
	}
}

/* Scan headers for poisoning */
func scanHeaders(client http.Client, headerList []string) {
	//c := make(chan result) //<- needed?
	sem := make(chan int, threads)
	var wg sync.WaitGroup
	wg.Add(len(headerList))

	for i, s := range headerList {
		if s == "" {
			if verbosity >= 2 {
				log.Printf("Skipping empty header (%d/%d) %s\n", i+1, len(headerList), s)
			}
			wg.Done()
			continue
		}

		poison := randInt()

		go func(i int, s string, poison string) {
			defer wg.Done()
			sem <- 1

			s = strings.Trim(s, "\r")

			if verbosity >= 2 {
				log.Printf("Testing now (%d/%d) %s\n", i+1, len(headerList), s)
			}

			urlCb, cb := addCacheBuster(website.Url.String(), "")
			var req *http.Request
			var err error
			if doPost {
				req, err = http.NewRequest("POST", urlCb, bytes.NewBufferString(body))
			} else {
				req, err = http.NewRequest("GET", urlCb, nil)
			}
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			setRequest(req, doPost)

			if h := req.Header.Get(s); h != "" {
				log.Printf("Overwriting %s:%s with %s:%s\n", s, h, s, poison)
				req.Header.Set(s, poison)
			} else {
				req.Header.Add(s, poison)
			}

			resp, err := client.Do(req)
			if err != nil {
				<-sem
				log.Println(s, err)
				req.Header.Del(s)
				return
			}
			req.Header.Del(s)

			if resp.StatusCode != website.StatusCode {
				log.Printf("Unexpected Status Code %d for header %s=%s\n", resp.StatusCode, s, poison)
			}

			defer resp.Body.Close()
			bodyPoison, err := ioutil.ReadAll(resp.Body)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			if string(bodyPoison) == website.Body {
				<-sem
				return
			}

			if doPost {
				req, err = http.NewRequest("POST", urlCb, bytes.NewBufferString(body))
			} else {
				req, err = http.NewRequest("GET", urlCb, nil)
			}
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			setRequest(req, doPost)
			resp, err = client.Do(req)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}
			defer resp.Body.Close()

			if resp.StatusCode != website.StatusCode {
				log.Printf("Unexpected Status Code %d for header %s\n", resp.StatusCode, s)
			}

			bodyVictim, err := ioutil.ReadAll(resp.Body)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			if strings.Contains(string(bodyVictim), poison) {
				log.Println("")
				log.Printf("------- Header %s was successfully poisoned!!! cb: %s poison: %s -------\n", s, cb, poison)
			}

			<-sem
		}(i, s, poison)

	}
	wg.Wait()
}

/* Scan query parameters for poisoning */
func scanParameters(client http.Client, parameterList []string) {
	//c := make(chan result) //<- needed?
	sem := make(chan int, threads)
	var wg sync.WaitGroup
	wg.Add(len(parameterList))

	for i, s := range parameterList {
		if s == "" {
			if verbosity >= 2 {
				log.Printf("Skipping empty query (%d/%d) %s\n", i+1, len(parameterList), s)
			}
			wg.Done()
			continue
		}

		poison := randInt()

		go func(i int, s string, poison string) {
			defer wg.Done()
			sem <- 1

			s = strings.Trim(s, "\r")

			if verbosity >= 2 {
				log.Printf("Testing now simple fat GET (%d/%d) %s\n", i+1, len(parameterList), s)
			}

			var urlCb, cb string
			if _, ok := website.Queries[s]; ok {
				// if the query to add is already present
				queryParameterMap := make(map[string]string)

				for key, val := range website.Queries {
					queryParameterMap[key] = val
				}

				log.Printf("Overwriting %s=%s with %s=%s\n", s, queryParameterMap[s], s, poison)
				queryParameterMap[s] = poison

				urlCb = website.BaseUrlStr + "?"
				for key, val := range queryParameterMap {
					if !strings.HasSuffix(urlCb, "?") {
						urlCb += "&"
					}
					urlCb += key + "=" + val
				}

				urlCb, cb = addCacheBuster(urlCb+querySeperator, "")
			} else {
				// if query isn't already present, just add it and the cachebuster
				urlCb = website.Url.String()
				urlCb += s + "=" + poison + querySeperator
				urlCb, cb = addCacheBuster(urlCb, "")
			}

			var req *http.Request
			var err error
			if doPost {
				req, err = http.NewRequest("POST", urlCb, bytes.NewBufferString(body))
			} else {
				req, err = http.NewRequest("GET", urlCb, nil)
			}
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			setRequest(req, doPost)
			resp, err := client.Do(req)
			if err != nil {
				<-sem
				log.Println(s, err)
				return
			}
			defer resp.Body.Close()

			if resp.StatusCode != website.StatusCode {
				log.Printf("Unexpected Status Code %d for parameter %s=%s\n", resp.StatusCode, s, poison)
			}

			bodyPoison, err := ioutil.ReadAll(resp.Body)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			// check if something changed
			if string(bodyPoison) == website.Body {
				<-sem
				return
			}

			impactfulQueries = append(impactfulQueries, s)

			// get urlCb with the cachebuster but without the poisoned query
			urlCb, cb = addCacheBuster(website.Url.String(), cb)

			if doPost {
				req, err = http.NewRequest("POST", urlCb, bytes.NewBufferString(body))
			} else {
				req, err = http.NewRequest("GET", urlCb, nil)
			}
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			setRequest(req, doPost)
			resp, err = client.Do(req)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}
			defer resp.Body.Close()

			if resp.StatusCode != website.StatusCode {
				log.Printf("Unexpected Status Code %d for parameter %s\n", resp.StatusCode, s)
			}

			bodyVictim, err := ioutil.ReadAll(resp.Body)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			if strings.Contains(string(bodyVictim), poison) {
				log.Println("")
				log.Printf("------- Query Parameter %s was successfully poisoned!!! cb: %s poison: %s -------\n", s, cb, poison)
			}

			<-sem
		}(i, s, poison)

	}
	wg.Wait()
}

/* Check for fat GET */
func scanFatGET(client http.Client) {

	if len(impactfulQueries) == 0 {
		log.Println("No impactful query parameters were found beforehand. Run the query parameter scan (maybe with a different wordlist).")
		return
	}

	//c := make(chan result) //<- needed?
	sem := make(chan int, threads)
	var wg sync.WaitGroup
	wg.Add(len(impactfulQueries))

	for i, s := range impactfulQueries {

		poison := randInt()

		// basic fat get technique
		go func(i int, s string, poison string) {
			defer wg.Done()
			sem <- 1

			if verbosity >= 2 {
				log.Printf("Testing now simple fat GET (%d/%d) %s\n", i+1, len(impactfulQueries), s)
			}

			urlCb, cb := addCacheBuster(website.Url.String(), "")

			var req *http.Request
			var err error

			req, err = http.NewRequest("GET", urlCb, bytes.NewBufferString(s+"="+poison))
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			setRequest(req, doPost)
			resp, err := client.Do(req)
			if err != nil {
				<-sem
				log.Println(s, err)
				return
			}
			defer resp.Body.Close()

			if resp.StatusCode != website.StatusCode {
				log.Printf("Unexpected Status Code %d for parameter %s\n", resp.StatusCode, s)
			}

			body, err := ioutil.ReadAll(resp.Body)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			// check if something changed
			if string(body) == website.Body {
				<-sem
				return
			}

			// get urlCb with the cachebuster but without the poisoned query
			urlCb, cb = addCacheBuster(website.Url.String(), cb)

			req, err = http.NewRequest("GET", urlCb, nil)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			setRequest(req, doPost)
			resp, err = client.Do(req)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}
			defer resp.Body.Close()

			if resp.StatusCode != website.StatusCode {
				log.Printf("Unexpected Status Code %d for parameter %s\n", resp.StatusCode, s)
			}

			body, err = ioutil.ReadAll(resp.Body)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			if strings.Contains(string(body), poison) {
				log.Println("")
				log.Printf("------- Query Parameter %s was successfully poisoned via simple fat GET!!! cb: %s poison:%s -------\n", s, cb, poison)
			}

			<-sem
		}(i, s, poison)
	}
	wg.Wait()
	wg.Add(len(impactfulQueries))
	log.Println()
	for i, s := range impactfulQueries {

		poison := randInt()

		// X-HTTP-Method-Override fat get technique
		go func(i int, s string, poison string) {
			defer wg.Done()
			sem <- 1

			if verbosity >= 2 {
				log.Printf("Testing now X-HTTP-Method-Override fat GET (%d/%d) %s\n", i+1, len(impactfulQueries), s)
			}

			urlCb, cb := addCacheBuster(website.Url.String(), "")

			var req *http.Request
			var err error

			req, err = http.NewRequest("GET", urlCb, bytes.NewBufferString(s+"="+poison))
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			//true so ContentType will be set
			setRequest(req, true)

			if h := req.Header.Get("X-HTTP-Method-Override"); h != "" {
				log.Printf("Overwriting X-HTTP-Method-Override: %s with X-HTTP-Method-Override:%s\n", h, poison)
				req.Header.Set("X-HTTP-Method-Override", "POST")
			} else {
				req.Header.Add("X-HTTP-Method-Override", "POST")
			}

			resp, err := client.Do(req)
			if err != nil {
				<-sem
				log.Println(s, err)
				return
			}
			defer resp.Body.Close()

			if resp.StatusCode != website.StatusCode {
				log.Printf("Unexpected Status Code %d for parameter %s\n", resp.StatusCode, s)
			}

			body, err := ioutil.ReadAll(resp.Body)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			// check if something changed
			if string(body) == website.Body {
				<-sem
				return
			}

			// get urlCb with the cachebuster but without the poisoned query
			urlCb, cb = addCacheBuster(website.Url.String(), cb)

			req, err = http.NewRequest("GET", urlCb, nil)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			setRequest(req, doPost)
			resp, err = client.Do(req)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}
			defer resp.Body.Close()

			if resp.StatusCode != website.StatusCode {
				log.Printf("Unexpected Status Code %d for parameter %s\n", resp.StatusCode, s)
			}

			body, err = ioutil.ReadAll(resp.Body)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			if strings.Contains(string(body), poison) {
				log.Println("")
				log.Printf("------- Query Parameter %s was successfully poisoned via X-HTTP-Method-Override fat GET!!! cb:%s poison:%s-------\n", s, cb, poison)
			}

			<-sem
		}(i, s, poison)
	}
	wg.Wait()
	wg.Add(len(impactfulQueries))
	log.Println()
	for i, s := range impactfulQueries {

		poison := randInt()

		// basic fat get technique. Also add go func for POST technique and X-Override-HTTP-Method:POST technique
		go func(i int, s string, poison string) {
			defer wg.Done()
			sem <- 1

			if verbosity >= 2 {
				log.Printf("Testing now POST fat GET (%d/%d) %s\n", i+1, len(impactfulQueries), s)
			}

			urlCb, cb := addCacheBuster(website.Url.String(), "")

			var req *http.Request
			var err error

			req, err = http.NewRequest("POST", urlCb, bytes.NewBufferString(s+"="+poison))
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			// True so ContentType will be set
			setRequest(req, true)

			resp, err := client.Do(req)
			if err != nil {
				<-sem
				log.Println(s, err)
				return
			}
			defer resp.Body.Close()

			if resp.StatusCode != website.StatusCode {
				log.Printf("Unexpected Status Code %d for parameter %s\n", resp.StatusCode, s)
			}

			body, err := ioutil.ReadAll(resp.Body)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			// check if something changed
			if string(body) == website.Body {
				<-sem
				return
			}

			// get urlCb with the cachebuster but without the poisoned query
			urlCb, cb = addCacheBuster(website.Url.String(), cb)

			req, err = http.NewRequest("GET", urlCb, nil)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			setRequest(req, doPost)
			resp, err = client.Do(req)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}
			defer resp.Body.Close()

			if resp.StatusCode != website.StatusCode {
				log.Printf("Unexpected Status Code %d for parameter %s\n", resp.StatusCode, s)
			}

			body, err = ioutil.ReadAll(resp.Body)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			if strings.Contains(string(body), poison) {
				log.Println("")
				log.Printf("------- Query Parameter %s was successfully poisoned via simple fat GET!!! cb:%s poison:%s -------\n", s, cb, poison)
			}

			<-sem
		}(i, s, poison)

	}
	wg.Wait()
}

/* Check for fat GET */
func scanParameterCloaking(client http.Client) {

	if len(impactfulQueries) == 0 {
		log.Println("No impactful query parameters were found beforehand. Run the query parameter scan (maybe with a different wordlist).")
		return
	}

	utm_parameter := []string{"utm_source", "utm_medium", "utm_campaign", "utm_content", "utm_term"}
	unkeyed_parameter := []string{}

	urlCb, _ := addCacheBuster(website.Url.String(), "")

	var req *http.Request
	var err error

	/***********Check if urlCb already contains utm parameter.
				Check if ? or querySeperator is needed
	****************/
	req, err = http.NewRequest("GET", urlCb, nil)
	if err != nil {
		log.Fatalln("first request", err)
	}

	setRequest(req, doPost)
	resp, err := client.Do(req)
	if err != nil {
		log.Println("first request", err)
		return
	}
	defer resp.Body.Close()

	if resp.StatusCode != website.StatusCode {
		log.Printf("Unexpected Status Code %d for first request\n", resp.StatusCode)
	}

	//c := make(chan result) //<- needed?
	sem := make(chan int, threads)
	var wg sync.WaitGroup
	wg.Add(len(utm_parameter))

	for i, s := range utm_parameter {
		go func(i int, s string) {
			defer wg.Done()
			sem <- 1

			if verbosity >= 2 {
				log.Printf("Testing now for unkeyed query parameters (%d/%d) %s\n", i+1, len(impactfulQueries), s)
			}

			var req *http.Request
			var err error

			req, err = http.NewRequest("GET", urlCb, nil)
			if err != nil {
				<-sem
				log.Fatalln(s, err)
			}

			setRequest(req, doPost)
			resp, err := client.Do(req)
			if err != nil {
				<-sem
				log.Println(s, err)
				return
			}
			defer resp.Body.Close()

			//TODO: TEST FOR UNEXPECTED RESPONSE CODE

			/************** if cache hit ************/
			if true {
				unkeyed_parameter = append(unkeyed_parameter, s)
			}

		}(i, s)
	}
	wg.Wait()

	/********************************************************/
	unkeyed_parameter = append(unkeyed_parameter, "utm_content")
	/********************************************************/

	if len(unkeyed_parameter) == 0 {
		log.Println("No unkeyed utm parameters could be found. Parameter Cloaking is not possible using utm parameters")
	} else {
		log.Printf("The following utm parameters were found to be unkeyed and will be now testet for parameter cloaking: %s\n", unkeyed_parameter)
	}

	cloak := ";"
	if querySeperator == ";" {
		cloak = "&"
	}

	wg.Add(len(impactfulQueries))

	for iu, u := range unkeyed_parameter {
		for is, s := range impactfulQueries {

			poison := randInt()

			go func(iu int, u string, is int, s string, poison string) {
				defer wg.Done()
				sem <- 1

				if verbosity >= 2 {
					log.Printf("Testing now Parameter Cloaking (%d/%d) %s%s%s\n", iu+is+1, len(impactfulQueries)*len(unkeyed_parameter), u, cloak, s)
				}

				urlCb, cb := addCacheBuster(website.Url.String(), "")

				var req *http.Request
				var err error

				/***************	add u=test cloak s=poison
				**************/
				req, err = http.NewRequest("GET", urlCb, nil)
				if err != nil {
					<-sem
					log.Fatalln(s, err)
				}

				setRequest(req, doPost)
				resp, err := client.Do(req)
				if err != nil {
					<-sem
					log.Println(s, err)
					return
				}
				defer resp.Body.Close()

				if resp.StatusCode != website.StatusCode {
					log.Printf("Unexpected Status Code %d for parameter %s\n", resp.StatusCode, s)
				}

				body, err := ioutil.ReadAll(resp.Body)
				if err != nil {
					<-sem
					log.Fatalln(s, err)
				}

				// check if something changed
				if string(body) == website.Body {
					<-sem
					return
				}

				// get urlCb with the cachebuster but without the poisoned query
				urlCb, cb = addCacheBuster(website.Url.String(), cb)

				req, err = http.NewRequest("GET", urlCb, nil)
				if err != nil {
					<-sem
					log.Fatalln(s, err)
				}

				setRequest(req, doPost)
				resp, err = client.Do(req)
				if err != nil {
					<-sem
					log.Fatalln(s, err)
				}
				defer resp.Body.Close()

				if resp.StatusCode != website.StatusCode {
					log.Printf("Unexpected Status Code %d for parameter %s\n", resp.StatusCode, s)
				}

				body, err = ioutil.ReadAll(resp.Body)
				if err != nil {
					<-sem
					log.Fatalln(s, err)
				}

				if strings.Contains(string(body), poison) {
					log.Println("")
					log.Printf("------- Query Parameter %s was successfully poisoned via simple fat GET!!! cb:%s poison:%s -------\n", s, cb, poison)
				}

				<-sem
			}(iu, u, is, s, poison)
		}
	}
	wg.Wait()
}
